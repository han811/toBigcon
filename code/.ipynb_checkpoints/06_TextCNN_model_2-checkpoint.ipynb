{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRu5ntHc31Cv"
   },
   "source": [
    "# Text CNN\n",
    "상품명 정보 등을 활용한 입력값으로 하는 Text CNN 모델.\n",
    "TextCNN 뒤에 dense layer가 여러개 추가되는 형태.\n",
    "\n",
    "1) Input  \n",
    ": 시간 정보, 마더 코드, 상품군, 상품명.\n",
    "상품명 하나를 집어넣기보다는 시간 정보 등을 넣어줌. \n",
    "word embedding의 값은 표준정규분포를 따르도록 초기화됨.\n",
    "\n",
    "2) Convolutions and MaxPooling  \n",
    ": Convolution 연산을 통해서 텍스트의 특징과 패턴을 추출함. 이후 MaxPooling을 통해서 가장 뚜렷한 특징을 추출.\n",
    "\n",
    "3) FC Layer  \n",
    ": FC layer 4개를 통과하여 추출해낸 특징의 분포를 다양하게 탐색하며, 취급액의 분포를 근사."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHqJmh-GIqWA"
   },
   "source": [
    "## Contents\n",
    "\n",
    "- Data 준비  \n",
    "- Dataset 구축    \n",
    "- Model 정의  \n",
    "- Model Training  \n",
    "- Model Evaluation  \n",
    "- Predict Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gElfkLyvJanD"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HIemTMir3n8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmiYYxveJZaJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ1A3TfgWk0O"
   },
   "outputs": [],
   "source": [
    "from torchtext import data as td\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "E7DVqvcE6ngX",
    "outputId": "cffaacf2-38bf-4410-8f7a-dcf1a171a46d"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qz3Rw_jYr3oC",
    "outputId": "f178001d-c47d-44b6-94e0-03fb83e36784"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # random seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XVtW2VqEKgl1",
    "outputId": "dc762607-5ccb-4ac4-c2d1-2cefe0b51f23"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3qId_KBKjKF"
   },
   "outputs": [],
   "source": [
    "data_path = \"../data/\" # 데이터 경로\n",
    "output_path = \"../data\" # prediction 저장 경로\n",
    "model_path = \"../data\" # 모델 저장 경로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opxL7OvYNzh6"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJYrosdhQSa6"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, \"df_0927_yujin.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ptb-sVM3Mj8k",
    "outputId": "ee2487ab-6bb7-4a48-daf0-7435c655ae19"
   },
   "outputs": [],
   "source": [
    "print(f\"DataFrame Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv9yZXcnQOrj"
   },
   "source": [
    "## Data 준비\n",
    ": 월, 요일, 시간, 마더코드, 상품군, 소분류, 상품명을 하나의 string으로 묶어서 input으로 준비  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDOTwusGQtLb"
   },
   "outputs": [],
   "source": [
    "df[\"마더코드\"] = df[\"마더코드\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "pJU9Bulgx1EH",
    "outputId": "b2cec043-fa90-4514-9fda-acf6ededca07"
   },
   "outputs": [],
   "source": [
    "tmp = df[\"방송일시\"].str.replace(\"-\",\"\").apply(lambda x: x[4:-3]).str.replace(\":\", \" \").apply(lambda x: x[:2] + \"월\" + x[4:7] + \"시\" + x[7:] + \"분\")\n",
    "text_time = tmp + \" \" + df[\"요일\"] + \" \" + df[\"마더코드\"] + \" \" + df[\"상품군\"] + \" \" + df[\"소분류\"] + \" \" + df[\"상품명\"]\n",
    "text_time = text_time.str.strip()\n",
    "\n",
    "text_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG9WVZTQSDvL"
   },
   "source": [
    "### token length 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "MfQPaHHGRAoB",
    "outputId": "795c3b59-fbbc-46f7-c76d-4f45e2239a9a"
   },
   "outputs": [],
   "source": [
    "tokens = text_time.str.split(\" \")\n",
    "token_length = tokens.apply(lambda x: len(x))\n",
    "plt.hist(token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VWQKdP3S6_T"
   },
   "source": [
    "## Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35pRtVYaTkHF"
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.concat([text_time, df[[\"상품군\",\"취급액\"]]], axis = 1)\n",
    "df_tmp.columns = [\"상품정보\", \"상품군\", \"취급액\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pvQXkXMUNwj"
   },
   "outputs": [],
   "source": [
    "train_data = df_tmp[df_tmp[\"취급액\"] != -1]\n",
    "test_data = df_tmp[df_tmp[\"취급액\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MVPDC_vRHsN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "grp_dct = {v:k for k, v in enumerate(train_data[\"상품군\"].unique())}\n",
    "grp_idx = train_data[\"상품군\"].map(grp_dct)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data[\"상품정보\"],train_data['취급액'].astype(float), random_state = 0, stratify = grp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "mhw7pOD8Ssxz",
    "outputId": "ba76a304-6d16-49a8-d946-23cbec2e279b"
   },
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_val.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aM5zWDSSRN6C"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([X_train, y_train]).T.to_csv(os.path.join(data_path, \"text_train.csv\"), index = False)\n",
    "pd.DataFrame([X_val, y_val]).T.to_csv(os.path.join(data_path, \"text_valid.csv\"), index = False)\n",
    "train_data[[\"상품정보\", \"취급액\"]].to_csv(os.path.join(data_path, \"text_whole_train.csv\"), index = False)\n",
    "test_data[[\"상품정보\", \"취급액\"]].to_csv(os.path.join(data_path, \"text_test.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1-L7ifZr3qJ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "LMYJpClX8qik",
    "outputId": "782c37b4-bee9-440f-93ec-b4b2eee8c915"
   },
   "outputs": [],
   "source": [
    "# Model Parameter\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.003\n",
    "N_EPOCHS = 100\n",
    "WEIGHT_DECAY = 1e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Model Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Model Learning Rate: {LR}\")\n",
    "print(f\"Model Number Of Epochs: {N_EPOCHS}\")\n",
    "print(f\"Model Weight Decay Rate: {WEIGHT_DECAY}\")\n",
    "print(f\"Model Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSD3K6bxr3qK"
   },
   "source": [
    "### Dataset 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxVnlVK4r3qZ"
   },
   "outputs": [],
   "source": [
    "# field 정의\n",
    "\n",
    "PRODUCT = td.Field(sequential = True, \n",
    "                   use_vocab = True, \n",
    "                   batch_first = True)\n",
    "\n",
    "SALES = td.Field(sequential = False,\n",
    "                 use_vocab = False,\n",
    "                 preprocessing = lambda x: float(x),\n",
    "                 dtype = torch.float,\n",
    "                 batch_first = True)\n",
    "\n",
    "fields = [('text', PRODUCT), ('target', SALES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQUiFKblr3qe"
   },
   "outputs": [],
   "source": [
    "# data 불러오기\n",
    "\n",
    "train_data,val_data = td.TabularDataset.splits(\n",
    "                        path = data_path,\n",
    "                        train = \"text_train.csv\",\n",
    "                        validation = \"text_valid.csv\",                       \n",
    "                        format = \"csv\",\n",
    "                        fields = fields,\n",
    "                        skip_header = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NzTAXA12-oUK",
    "outputId": "22967fbc-b7e5-4111-e538-72279f3bda26"
   },
   "outputs": [],
   "source": [
    "PRODUCT.build_vocab(train_data)\n",
    "print(f\"Vocabulary Size: {len(PRODUCT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaqVtHW6r3qx"
   },
   "outputs": [],
   "source": [
    "# Batch iterator\n",
    "\n",
    "train_iterator, valid_iterator = td.BucketIterator.splits(\n",
    "(train_data, val_data),\n",
    "    sort = False,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e7JT_vUXKwu"
   },
   "source": [
    "### 모델 아키텍처 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdT9pv-Br3rA"
   },
   "outputs": [],
   "source": [
    "# code reference: https://github.com/kh-kim/simple-ntc/blob/master/simple_ntc/cnn.py\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    - Input\n",
    "    1) input_size: Vocabulary Size (int)\n",
    "    2) word_vec_dim: Embedding Dimension (int)\n",
    "    3) dropout_p: Dropout Rate (float)\n",
    "    4) window_sizes: A list of window sizes (list)\n",
    "    5) n_filters: A list of which the number of filters for each window size (list)\n",
    "    6) hidden_sizes: hidden layer sizes (list)\n",
    "\n",
    "    window size means that how many words a single pattern covers,\n",
    "    and n_filter means that how many patterns to cover.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 word_vec_dim,\n",
    "                 dropout_p = .5,\n",
    "                 window_sizes = [3,4,5],\n",
    "                 n_filters = [100,100,100],\n",
    "                 hidden_sizes = [1]\n",
    "                ):\n",
    "        self.input_size = input_size\n",
    "        self.word_vec_dim = word_vec_dim\n",
    "        self.dropout_p = dropout_p\n",
    "        self.window_sizes = window_sizes\n",
    "        self.n_filters = n_filters\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(input_size, word_vec_dim)\n",
    "        \n",
    "        # convolution layer 개수는 window_sizes 개수에 따라 달라지므로, layer를 더하기 위해 setattr, getattr 메소드를 사용할 것\n",
    "        # 이 layer가 sequential하게 이어지는 게 아니라 각각 독립적인 convolution block.\n",
    "        # layer 개수는 window_sizes*n_filters의 가중합.\n",
    "        # setattr: 속성 할당 ex) setattr(obj, attr_name, attr_value)\n",
    "        # getattr: 속성 반환 ex) getattr(obj, attr_name, value to be returned when attr doesnt exist(option))\n",
    "        \n",
    "        for window_size, n_filter in zip(window_sizes, n_filters): # convolution layers\n",
    "            cnn = nn.Conv2d(in_channels = 1, \n",
    "                           out_channels = n_filter, \n",
    "                           kernel_size = (window_size, word_vec_dim)\n",
    "                           )\n",
    "            \n",
    "            setattr(self, \"cnn-%d-%d\" % (window_size, n_filter), cnn)\n",
    "\n",
    "        for idx in range(len(hidden_sizes)): # hidden layers\n",
    "            if idx == 0:\n",
    "              dim0 = sum(n_filters)\n",
    "            else:\n",
    "              dim0 = hidden_sizes[idx-1]\n",
    "            \n",
    "            affine = nn.Linear(dim0, hidden_sizes[idx])\n",
    "            \n",
    "            setattr(self, \"linear-%d-%d\" % (dim0, hidden_sizes[idx]), affine)\n",
    "\n",
    "            \n",
    "        self.relu = nn.ReLU() # activation layer\n",
    "        self.dropout = nn.Dropout(dropout_p) # dropout\n",
    "        self.generator = nn.Linear(hidden_sizes[idx], 1) # output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input -> (embedding) -> Convolution (window_sizes * n_filters) -> ReLU -> Dropout -> max_pooling1D -> Dense -> Output\n",
    "\n",
    "        - Tracking the changing size\n",
    "        1) input: |x| = (batch_size, length)\n",
    "        2) After Embedding: |x| = (batch_size, length, word_vec_dim)\n",
    "        3) After Convolution: |cnn_out| = (batch_size, n_filter, length - window_size + 1, 1)\n",
    "        4) After MaxPooling: |cnn_out| = (batch_size, n_filters)\n",
    "        5) After Concat: |cnn_outs| = (batch_size, sum(n_filters))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Embedding Layers\n",
    "        x = self.emb(x)\n",
    "        min_length = max(self.window_sizes)\n",
    "        \n",
    "        ## Window sizes보다 Sequence Length의 길이가 작을 때를 대비하여 zero padding\n",
    "        if min_length > x.size(1): \n",
    "            pad = x.new(x.size(0), min_length - x.size(1), self.word_vec_dim).zero_()\n",
    "            x = torch.cat([x, pad], dim = 1) \n",
    "      \n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Conv Layers\n",
    "        cnn_outs = []\n",
    "        for window_size, n_filter in zip(self.window_sizes, self.n_filters):\n",
    "            cnn = getattr(self, \"cnn-%d-%d\" % (window_size, n_filter))\n",
    "            cnn_out = self.dropout(self.relu(cnn(x)))\n",
    "          \n",
    "            # MaxPooling Layers          \n",
    "            cnn_out = nn.functional.max_pool1d(input = cnn_out.squeeze(-1),\n",
    "                                              kernel_size = cnn_out.size(-2)\n",
    "                                              ).squeeze(-1)\n",
    "            \n",
    "            cnn_outs += [cnn_out] # len(cnn_out) for each: number of feature map\n",
    "        \n",
    "        cnn_outs = torch.cat(cnn_outs, dim = -1) # hstack\n",
    "        \n",
    "        # Dense Layers\n",
    "        affine_in = cnn_outs\n",
    "        for idx in range(len(self.hidden_sizes)):\n",
    "          if idx == 0:\n",
    "            dim0 = sum(self.n_filters)\n",
    "          else:\n",
    "            dim0 = self.hidden_sizes[idx-1]\n",
    "          affine = getattr(self, \"linear-%d-%d\" % (dim0, self.hidden_sizes[idx]))\n",
    "          affine_out = self.dropout(self.relu(affine(affine_in)))\n",
    "          affine_in = affine_out\n",
    "\n",
    "        # Output\n",
    "        y = self.generator(affine_out)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4p1FMisr3rH"
   },
   "source": [
    "### Model Train\n",
    ": MAPE 정의, train & evaluate 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cxFSftOr3rW"
   },
   "outputs": [],
   "source": [
    "def mape(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Loss Function & Eval metric at the same time.\n",
    "    Returns mape\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs((y_true-y_pred)/y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JkudzH4r3rb"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    model training\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_mape = 0\n",
    "    \n",
    "    model.train() # train\n",
    "    \n",
    "    for batch in iterator:\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "        loss_ = mape(predictions, batch.target)\n",
    "        mape_ = mape(predictions, batch.target)\n",
    "        \n",
    "        loss_.backward() # backpropagation\n",
    "        optimizer.step() # updating parameters\n",
    "        \n",
    "        epoch_loss += loss_.item()\n",
    "        epoch_mape += mape_.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_mape / len(iterator)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e0bzbvfr3rg"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    \"\"\"\n",
    "    evaluating model\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_mape = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in iterator:\n",
    "            \n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss_ = mape(predictions, batch.target)\n",
    "            mape_ = mape(predictions, batch.target)\n",
    "    \n",
    "            \n",
    "            epoch_loss += loss_.item()\n",
    "            epoch_mape += mape_.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_mape / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH2a7D6Wr3rn"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    \"\"\"\n",
    "    check epoch time\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OP1PVzEr3qP"
   },
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "\"\"\"\n",
    "ReduceLROnPlateau: loss가 더이상 감소하지 않으면 learning rate를 조정함.\n",
    "factor: learning rate decay rate\n",
    "patience: after steps of patience, learning rate decaying started\n",
    "\"\"\"\n",
    "model = TextCNN(len(PRODUCT.vocab), \n",
    "                32,\n",
    "                0.1,\n",
    "                window_sizes = [1,2,3,4,5,6,7,8,9],\n",
    "                n_filters = [15,15,30,35,35,30,15,15,15],\n",
    "                hidden_sizes = [64,16,4])\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR, weight_decay = WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=10, verbose=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2KCGPs_ar3ru",
    "outputId": "457c43a8-be56-4fc5-faa5-40c40a4f8f7f"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "save_checkpoint = False # whether to save checkpoint\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_mape = train(model, train_iterator, optimizer)\n",
    "    valid_loss, valid_mape = evaluate(model, valid_iterator)\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "      best_valid_loss = valid_loss\n",
    "      \n",
    "      ## save the checkpoint\n",
    "      if save_checkpoint:\n",
    "        torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': valid_loss\n",
    "              }, f\"{model_path}{today}-model_checkpoint.tar\")\n",
    "      else: ## save the model only\n",
    "        torch.save(model.state_dict(), f\"{model_path}{today}-model.pt\")\n",
    "      \n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train MAPE: {train_mape:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. MAPE: {valid_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "8eDbxAgCOeVk",
    "outputId": "792a2f19-6b7b-4665-ffbd-209f01b82604"
   },
   "outputs": [],
   "source": [
    "print(f\"best valid loss for {best_valid_loss} epochs : {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "bx-Nv8PIzz6W",
    "outputId": "a6937f83-2a72-42ed-9117-c966fd9e8b04"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Loss Plot\")\n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(valid_loss_list)\n",
    "plt.legend([\"train_loss\", \"validation_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5TyLs-2ocLz"
   },
   "source": [
    "### 전체 Train set으로 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oCZz7ZV1PFZ"
   },
   "outputs": [],
   "source": [
    "final_train_data = td.TabularDataset.splits(\n",
    "                        path = data_path,\n",
    "                        train = \"text_whole_train.csv\",\n",
    "                        #test = \"text_test.csv\",\n",
    "                        format = \"csv\",\n",
    "                        fields = fields,\n",
    "                        skip_header = True\n",
    "                      )[0] # tuple unpacking\n",
    "\n",
    "PRODUCT.build_vocab(final_train_data)\n",
    "whole_train_iterator = td.BucketIterator(final_train_data, batch_size = BATCH_SIZE, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "h5kTpbJjAgSW",
    "outputId": "c4e17bed-eb88-4645-fa89-95f993292842"
   },
   "outputs": [],
   "source": [
    "len(PRODUCT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EObXJTxookWm"
   },
   "outputs": [],
   "source": [
    "# 전체 트레인셋에 대해 학습 후 최종 예측 진행\n",
    "\n",
    "def final_train(train_iterator, n_epochs):\n",
    "  model = TextCNN(len(PRODUCT.vocab), \n",
    "                32,\n",
    "                0.1,\n",
    "                window_sizes = [1,2,3,4,5,6,7,8,9],\n",
    "                n_filters = [15,15,30,35,35,30,15,15,15],\n",
    "                hidden_sizes = [64,16,4])\n",
    "\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr = LR, weight_decay = WEIGHT_DECAY)\n",
    "  model = model.to(device)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_mape = train(model, train_iterator, optimizer)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train MAPE: {train_mape:.2f}%')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xX54yKHDt6dM",
    "outputId": "93fb1052-c085-40f1-9604-82ef423bdeb9"
   },
   "outputs": [],
   "source": [
    "models_list = [final_train(whole_train_iterator, epochs) for epochs in [20,30,40]] # 20,30,40 training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiCiMEXq4BEs"
   },
   "source": [
    "## Make A Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyW-zwwQ4zJd"
   },
   "outputs": [],
   "source": [
    "test_final = pd.read_csv(os.path.join(data_path, \"text_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4m_VR14J5sxg"
   },
   "outputs": [],
   "source": [
    "def get_idx_tokens(data):\n",
    "  \"\"\"\n",
    "  Getting index from text data\n",
    "  \"\"\"\n",
    "\n",
    "  tokens = data.iloc[:,0].str.split(\" \")\n",
    "  max_len =tokens.apply(len).max()\n",
    "  word_to_idx = dict(PRODUCT.vocab.stoi)\n",
    "  \n",
    "  def get_matching_idx(lst):\n",
    "    res = []\n",
    "    for i in lst:\n",
    "      if i in word_to_idx.keys():\n",
    "        res.append(word_to_idx[i])\n",
    "      else:\n",
    "        res.append(0)\n",
    "    return res\n",
    "  token_idx = tokens.apply(lambda x: get_matching_idx(x))\n",
    "  token_idx_padded = token_idx.apply(lambda x: x + [1] * (max_len - len(x)))\n",
    "\n",
    "  return torch.LongTensor(token_idx_padded).to(device)\n",
    "\n",
    "def get_predict(seq, model):\n",
    "  \"\"\"\n",
    "  make predictions\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "\n",
    "    predictions = model(seq).squeeze(1)\n",
    "\n",
    "  return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pru5X8CDSinm"
   },
   "outputs": [],
   "source": [
    "# get_predict\n",
    "val_padded = get_idx_tokens(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Dtc2HFhEjXC"
   },
   "outputs": [],
   "source": [
    "unk_mask = pd.Series(val_padded.cpu()).apply(lambda x: x.count(0) / len(x) >= 0.3 ) # unknown token 비율이 0.3이상인 데이터를 걸러냄\n",
    "unk_mask_idx = np.where(unk_mask == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuxHI3G-Eiy2"
   },
   "outputs": [],
   "source": [
    "res = np.array([get_predict(val_padded, mdl) for mdl in models_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cO2_bDWU05F6"
   },
   "outputs": [],
   "source": [
    "res_avg = res.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbpGMuc6OVKn"
   },
   "outputs": [],
   "source": [
    "# Writing results\n",
    "\n",
    "test_final[\"취급액\"] = res_avg\n",
    "test_final[\"취급액\"].to_csv(os.path.join(output_path, \"text_cnn_final_predictions.csv\"), index = False)\n",
    "\n",
    "with open(os.path.join(output_path, \"unk_token_masking.pickle\"), \"wb\") as f:\n",
    "  pickle.dump(unk_mask_idx, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "opxL7OvYNzh6"
   ],
   "machine_shape": "hm",
   "name": "TextCNN_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
