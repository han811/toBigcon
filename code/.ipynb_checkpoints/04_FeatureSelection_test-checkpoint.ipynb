{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import datetime\n",
    "import locale                                         \n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "locale.setlocale(locale.LC_ALL, 'ko_KR.UTF-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/merged_data_concat_0926.pickle', 'rb') as f:\n",
    "    performance_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "today = datetime.today().strftime(\"%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = performance_data[performance_data.취급액 != -1]\n",
    "test = performance_data[performance_data.취급액 == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(test.마더코드.unique())))\n",
    "print(len(set(train.마더코드.unique())))\n",
    "print(len(set(test.마더코드.unique()) - set(train.마더코드.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(test.상품코드.unique())))\n",
    "print(len(set(train.상품코드.unique())))\n",
    "print(len(set(test.상품코드.unique()) - set(train.상품코드.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(test.상품명.unique())))\n",
    "print(len(set(train.상품명.unique())))\n",
    "print(len(set(test.상품명.unique()) - set(train.상품명.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(test.중분류.unique())))\n",
    "print(len(set(train.중분류.unique())))\n",
    "print(len(set(test.중분류.unique()) - set(train.중분류.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(test.소분류.unique())))\n",
    "print(len(set(train.소분류.unique())))\n",
    "print(len(set(test.소분류.unique()) - set(train.소분류.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코딩\n",
    "test_data = performance_data.drop(['방송일시','판매량', 'holiday', '대비', 'date', 'mean_rating', \n",
    "                                   '배당수익률(%)', '주가자산비율', '고가지수', '저가지수', '거래량(천주)', '거래대금(백만원)','상장시가총액(백만원)'],axis=1)\n",
    "\n",
    "# test_data['상품코드'] = test_data['상품코드'].map(int)\n",
    "for feat in ['상품명','상품코드','마더코드','prime_time','중분류','요일','season','남여','muil','브랜드','season_prod','소분류']:\n",
    "    lbe = LabelEncoder()\n",
    "    test_data[feat] = lbe.fit_transform(test_data[feat].astype(str).values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_group = test_data[\"상품군\"].unique()\n",
    "prod_group_dct = {v:k for k, v in enumerate(prod_group)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추후 전체 set에 대한 mape를 구하기 위해서 split을 해줌.\n",
    "\n",
    "train_set = test_data[test_data['취급액'] != -1]\n",
    "\n",
    "X = train_set.drop([\"취급액\"], axis = 1)\n",
    "y = train_set[\"취급액\"]\n",
    "\n",
    "grp_idx = train_set['상품군'].map(prod_group_dct)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify = grp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = [len(X_test[X_test[\"상품군\"] == prod_group[i]]) for i in range(len(prod_group))]\n",
    "print(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split된 걸 상품군 별로 나눔\n",
    "\n",
    "def train_test_grp(X_train, X_test, y_train, y_test, prod_group, grp_index):\n",
    "    new_X_train = X_train[X_train[\"상품군\"] == prod_group[grp_index]].drop(\"상품군\", axis=1)\n",
    "    new_X_test = X_test[X_test[\"상품군\"] == prod_group[grp_index]].drop(\"상품군\", axis=1)\n",
    "    new_y_train = y_train[X_train[\"상품군\"] == prod_group[grp_index]]\n",
    "    new_y_test = y_test[X_test[\"상품군\"] == prod_group[grp_index]]\n",
    "    return new_X_train, new_X_test, new_y_train, new_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold용 데이터 만들기\n",
    "def make_grp_data(X,idx):\n",
    "    X_data = X[X[\"상품군\"] == prod_group[idx]].drop([\"상품군\"], axis=1)\n",
    "#     y_data = y[X[\"상품군\"] == prod_group[idx]]['취급액']\n",
    "    return X_data\n",
    "\n",
    "clothes= make_grp_data(train_set,0)\n",
    "inner = make_grp_data(train_set,1)\n",
    "kitchen = make_grp_data(train_set,2)\n",
    "food = make_grp_data(train_set,3)\n",
    "beauty = make_grp_data(train_set,4)\n",
    "elec = make_grp_data(train_set,5)\n",
    "goods = make_grp_data(train_set,6)\n",
    "health = make_grp_data(train_set,7)\n",
    "etc = make_grp_data(train_set,8)\n",
    "furn = make_grp_data(train_set,9)\n",
    "bed = make_grp_data(train_set,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clothes, X_test_clothes, y_train_clothes, y_test_clothes = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 0)\n",
    "X_train_inner, X_test_inner, y_train_inner, y_test_inner = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 1)\n",
    "X_train_kitchen, X_test_kitchen, y_train_kitchen, y_test_kitchen = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 2)\n",
    "X_train_food, X_test_food, y_train_food, y_test_food = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 3)\n",
    "X_train_beauty, X_test_beauty, y_train_beauty, y_test_beauty = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 4)\n",
    "X_train_elec, X_test_elec, y_train_elec, y_test_elec = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 5)\n",
    "X_train_goods, X_test_goods, y_train_goods, y_test_goods = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 6)\n",
    "X_train_health, X_test_health, y_train_health, y_test_health = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 7)\n",
    "X_train_etc, X_test_etc, y_train_etc, y_test_etc = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 8)\n",
    "X_train_furn, X_test_furn, y_train_furn, y_test_furn = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 9)\n",
    "X_train_bed, X_test_bed, y_train_bed, y_test_bed = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['상품군'] = lbe.fit_transform(test_data['상품군'].astype(str).values)\n",
    "train_set = test_data[test_data['취급액'] != -1]\n",
    "\n",
    "X = train_set.drop([\"취급액\"], axis = 1)\n",
    "y = train_set[\"취급액\"]\n",
    "\n",
    "# grp_idx = train_set['상품군'].map(prod_group_dct)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify = grp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "X_train_clothes_scaled = pd.DataFrame(scaler.fit_transform(X_train_clothes))\n",
    "X_test_clothes_scaled = pd.DataFrame(scaler.transform(X_test_clothes))\n",
    "X_train_inner_scaled = pd.DataFrame(scaler.fit_transform(X_train_inner))\n",
    "X_test_inner_scaled = pd.DataFrame(scaler.transform(X_test_inner))\n",
    "X_train_kitchen_scaled = pd.DataFrame(scaler.fit_transform(X_train_kitchen))\n",
    "X_test_kitchen_scaled = pd.DataFrame(scaler.transform(X_test_kitchen))\n",
    "X_train_food_scaled = pd.DataFrame(scaler.fit_transform(X_train_food))\n",
    "X_test_food_scaled = pd.DataFrame(scaler.transform(X_test_food))\n",
    "X_train_beauty_scaled = pd.DataFrame(scaler.fit_transform(X_train_beauty))\n",
    "X_test_beauty_scaled = pd.DataFrame(scaler.transform(X_test_beauty))\n",
    "X_train_elec_scaled = pd.DataFrame(scaler.fit_transform(X_train_elec))\n",
    "X_test_elec_scaled = pd.DataFrame(scaler.transform(X_test_elec))\n",
    "X_train_goods_scaled = pd.DataFrame(scaler.fit_transform(X_train_goods))\n",
    "X_test_goods_scaled = pd.DataFrame(scaler.transform(X_test_goods))\n",
    "X_train_health_scaled = pd.DataFrame(scaler.fit_transform(X_train_health))\n",
    "X_test_health_scaled = pd.DataFrame(scaler.transform(X_test_health))\n",
    "X_train_etc_scaled = pd.DataFrame(scaler.fit_transform(X_train_etc))\n",
    "X_test_etc_scaled = pd.DataFrame(scaler.transform(X_test_etc))\n",
    "X_train_furn_scaled = pd.DataFrame(scaler.fit_transform(X_train_furn))\n",
    "X_test_furn_scaled = pd.DataFrame(scaler.transform(X_test_furn))\n",
    "X_train_bed_scaled = pd.DataFrame(scaler.fit_transform(X_train_bed))\n",
    "X_test_bed_scaled = pd.DataFrame(scaler.transform(X_test_bed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_colnames = list(X_train_clothes.columns)\n",
    "new_colnames = list(X_train_clothes_scaled.columns)\n",
    "colnames_dic = {}\n",
    "for i in range(len(original_colnames)):\n",
    "    colnames_dic[str(new_colnames[i])] = original_colnames[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return 100*np.mean(np.abs(y_pred - y_true) / y_true)\n",
    "\n",
    "best_tot_mape = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm1 = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, num_leaves = 23,verbose = 0, n_jobs = -1, objective = 'gamma')\n",
    "lgbm2 = LGBMRegressor(n_estimators = 2500,random_state = 0, max_depth = 11, learning_rate = 0.09, num_leaves = 23, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "lgbm3 = LGBMRegressor(n_estimators = 2500,random_state = 0, max_depth = 11, learning_rate = 0.09, num_leaves = 23, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "lgbm4 = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, num_leaves = 20, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "lgbm5 = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "lgbm6 = LGBMRegressor(n_estimators = 500,  num_leaves = 2048, boosting_type = \"dart\", random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'mape')\n",
    "lgbm7 = LGBMRegressor(n_estimators = 1200,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "lgbm8 = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "lgbm9 = LGBMRegressor(n_estimators = 1200, random_state = 0, boosting_type = 'dart', max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "lgbm10 = LGBMRegressor(n_estimators = 500, boosting_type = 'dart', num_leaves = 127, random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'mape', learning_rate = 0.09)\n",
    "lgbm11 = LGBMRegressor(n_estimators = 500,random_state = 0, max_depth = 11, num_leaves = 20, verbose = 0, n_jobs = -1, objective = 'tweedie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss에 variation을 줘보자  \n",
    "1) regression: mse loss  \n",
    "2) regression_l1: mae loss (group별로는 줄어드는데 오히려 전체에 대해서 mape를 계산하면 전체 mape는 높아짐)    \n",
    "3) fair:  \n",
    "4) huber:  \n",
    "5) poisson regression: 분포 모양이 어느 정도 비슷함. 근데 중요한 건 얜 y가 discrete임을 가정함. 뭐 얘로 해도 ㄱㅊ하게 예측하는 게 좀 있음.  \n",
    "6) quantile regression: 얘도 거지  \n",
    "7) mape: 거지같이 나옴(가전, 가구는 얘가 제일 나음)    \n",
    "8) gamma: ㄱㅊㄱㅊ 모양이 얘랑 비슷한 게 많음  \n",
    "9) tweedie: 얘는 0에 뭉치는 경향이 있다는 게 가장 뚜렷한 특징. BEST   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "\n",
    "pred1 = lgbm1.fit(X_train_clothes_scaled, y_train_clothes).predict(X_test_clothes_scaled)\n",
    "pred2 = lgbm2.fit(X_train_inner_scaled, y_train_inner).predict(X_test_inner_scaled)\n",
    "pred3 = lgbm3.fit(X_train_kitchen_scaled, y_train_kitchen).predict(X_test_kitchen_scaled)\n",
    "pred4 = lgbm4.fit(X_train_food_scaled, y_train_food).predict(X_test_food_scaled)\n",
    "pred5 = lgbm5.fit(X_train_beauty_scaled, y_train_beauty).predict(X_test_beauty_scaled)\n",
    "pred6 = lgbm6.fit(X_train_elec_scaled, y_train_elec).predict(X_test_elec_scaled)\n",
    "pred7 = lgbm7.fit(X_train_goods_scaled, y_train_goods).predict(X_test_goods_scaled)\n",
    "pred8 = lgbm8.fit(X_train_health_scaled, y_train_health).predict(X_test_health_scaled)\n",
    "pred9 = lgbm9.fit(X_train_etc_scaled, y_train_etc).predict(X_test_etc_scaled)\n",
    "pred10 = lgbm10.fit(X_train_furn_scaled, y_train_furn).predict(X_test_furn_scaled)\n",
    "pred11 = lgbm11.fit(X_train_bed_scaled, y_train_bed).predict(X_test_bed_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_negative_red(val):\n",
    "    \"\"\"\n",
    "    Takes a scalar and returns a string with\n",
    "    the css property `'color: red'` for negative\n",
    "    strings, black otherwise.\n",
    "    \"\"\"\n",
    "    if val < 0 :\n",
    "        color = 'red' \n",
    "    elif val == 0 :\n",
    "        color = 'blue'\n",
    "    else :\n",
    "        color ='black'\n",
    "    return 'color: %s' % color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_colnames = list(X_train.columns)\n",
    "new_colnames = list(X_train_scaled.columns)\n",
    "colnames_dic3 = {}\n",
    "for i in range(len(original_colnames)):\n",
    "    colnames_dic3[str(new_colnames[i])] = original_colnames[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames_dic2 = {v: k for k, v in colnames_dic.items()}\n",
    "colnames_dic4 = {v: k for k, v in colnames_dic3.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **전체**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = train_set.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_set.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled2 = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled2, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic4[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test,pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [33,10,37,35,16,18,9,14,15,21,24,17]\n",
    "lgbm = LGBMRegressor(n_estimators = 2500,random_state = 0, max_depth = 15, verbose = 0, n_jobs = -1, objective = 'gamma')\n",
    "pred_all = lgbm.fit(X_train_scaled.drop(del_columns,axis=1), y_train).predict(X_test_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test,pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **의류**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = clothes.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = clothes.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, num_leaves = 23,verbose = 0, n_jobs = -1, objective = 'gamma')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_clothes,pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [37,34,33,32,26,36,20,12,13,16,18]\n",
    "lgbm1 = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, num_leaves = 23,verbose = 0, n_jobs = -1, objective = 'gamma')\n",
    "pred1_new = lgbm1.fit(X_train_clothes_scaled.drop(del_columns,axis=1), y_train_clothes).predict(X_test_clothes_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_clothes,pred1_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37,34,33,32,26,36,20,12,13,16,18 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **속옷**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = inner.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = inner.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 2500,random_state = 0, max_depth = 11, learning_rate = 0.09, num_leaves = 23, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_inner,pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [34,33,32,26,36,37,28,17,18,9,27,12,13,25,22,8,28,14,19,15,16]\n",
    "lgbm2 = LGBMRegressor(n_estimators = 2500,random_state = 0, max_depth = 11, learning_rate = 0.09, num_leaves = 23, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "pred2_new = lgbm2.fit(X_train_inner_scaled.drop(del_columns,axis=1), y_train_inner).predict(X_test_inner_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_inner,pred2_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34,33,32,26,36,37,28,17,18,9,27,12,13,25,22,8,28,14,19,15,16 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **주방**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = kitchen.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = kitchen.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 2500,random_state = 0, max_depth = 11, learning_rate = 0.09, num_leaves = 23, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_kitchen,pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [25,32,34,36,33,27,9,13,8,17,15,18,16,12,28,14,11]\n",
    "lgbm3 = LGBMRegressor(n_estimators = 2500,random_state = 0, max_depth = 11, learning_rate = 0.09, num_leaves = 23, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "pred3_new = lgbm3.fit(X_train_kitchen_scaled.drop(del_columns,axis=1), y_train_kitchen).predict(X_test_kitchen_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_kitchen,pred3_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25,32,34,36,33,27,9,13,8,17,15,18,16,12,28,14,11 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **농수축**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = food.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = food.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, num_leaves = 20, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_food,pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [25,36,26,9,33,34,32,37,27,17,16,15,28,29,13,20,14]\n",
    "lgbm4 = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, num_leaves = 20, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "pred4_new = lgbm4.fit(X_train_food_scaled.drop(del_columns,axis=1), y_train_food).predict(X_test_food_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_food,pred4_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25,36,26,9,33,34,32,37,27,17,16,15,28,29,13,20,14 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **이미용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = beauty.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = beauty.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_beauty,pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [11,16,14,18,13,22,34,33,32,29,26,36,37,8,12,19,17,15]\n",
    "lgbm5 = LGBMRegressor(n_estimators = 500,random_state = 0, max_depth = 4, verbose = 0, n_jobs = -1, objective = 'gamma')\n",
    "pred5_new = lgbm5.fit(X_train_beauty_scaled.drop(del_columns,axis=1), y_train_beauty).predict(X_test_beauty_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_beauty,pred5_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11,16,14,18,13,22,34,33,32,29,26,36,37,8,12,19,17,15 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **가전**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = elec.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = elec.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 1000,  num_leaves = 1024, boosting_type = \"dart\", random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'mape')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_elec,pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [8,10,11,12,13,14,17,18,19,25,32,33,34,35,36]\n",
    "lgbm6 = LGBMRegressor(n_estimators = 1000,  num_leaves = 1024, boosting_type = \"dart\", random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'mape')\n",
    "pred6_new = lgbm6.fit(X_train_elec_scaled.drop(del_columns,axis=1), y_train_elec).predict(X_test_elec_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_elec,pred6_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8,10,11,12,13,14,17,18,19,25,32,33,34,35,36 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **생활용품**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = goods.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = goods.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 1200,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_goods,pred7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [13,14,15,16,17,19,20,21,22,23,25,29,32,33,34,37]\n",
    "lgbm7 = LGBMRegressor(n_estimators = 1200,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "pred7_new = lgbm7.fit(X_train_goods_scaled.drop(del_columns,axis=1), y_train_goods).predict(X_test_goods_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_goods,pred7_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13,14,15,16,17,19,20,21,22,23,25,29,32,33,34,37번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **건강기능**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = health.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = health.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_health,pred8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [8,11,13,14,15,16,17,18,20,25,26,27,32,33,34,36,37]\n",
    "lgbm8 = LGBMRegressor(n_estimators = 1500,random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "pred8_new = lgbm8.fit(X_train_health_scaled.drop(del_columns,axis=1), y_train_health).predict(X_test_health_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_health,pred8_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8,11,13,14,15,16,17,18,20,25,26,27,32,33,34,36,37 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **잡화**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = etc.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = etc.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 1200, random_state = 0, boosting_type = 'dart', max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_etc,pred9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [9,11,13,14,15,16,17,18,19,20,22,23,25,27,32,33,34,35,36,37]\n",
    "lgbm9 = LGBMRegressor(n_estimators = 1200, random_state = 0, boosting_type = 'dart', max_depth = 11, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "pred9_new = lgbm9.fit(X_train_etc_scaled.drop(del_columns,axis=1), y_train_etc).predict(X_test_etc_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_etc,pred9_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9,11,13,14,15,16,17,18,19,20,22,23,25,27,32,33,34,35,36,37 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **가구**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = furn.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = furn.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 500, boosting_type = 'dart', num_leaves = 127, random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'mape', learning_rate = 0.09)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_furn,pred10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [8,10,11,12,13,14,15,18,19,23,25,27,32,33,34,35,36]\n",
    "lgbm10 = LGBMRegressor(n_estimators = 500, boosting_type = 'dart', num_leaves = 127, random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'mape', learning_rate = 0.09)\n",
    "pred10_new = lgbm10.fit(X_train_furn_scaled.drop(del_columns,axis=1), y_train_furn).predict(X_test_furn_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_furn,pred10_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8,10,11,12,13,14,15,18,19,23,25,27,32,33,34,35,36 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **침구**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "CV = KFold(n_splits=5)\n",
    "FEATURES = bed.drop([\"취급액\"], axis=1).columns.tolist()\n",
    "TARGET_COL = \"취급액\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = bed.reset_index(drop=True).copy()\n",
    "for fold, (train_idx, valid_idx) in enumerate(CV.split(train_df, train_df[TARGET_COL])):\n",
    "    clf = LGBMRegressor(n_estimators = 500,random_state = 0, max_depth = 11, num_leaves = 20, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[train_idx, FEATURES]))\n",
    "    X_val_scaled = pd.DataFrame(scaler.fit_transform(train_df.loc[valid_idx, FEATURES]))\n",
    "    \n",
    "    clf.fit(X_train_scaled, \n",
    "            train_df.loc[train_idx, TARGET_COL], \n",
    "            verbose = 0,\n",
    "            early_stopping_rounds=1000,\n",
    "            eval_set=[(X_val_scaled, \n",
    "                       train_df.loc[valid_idx, TARGET_COL])])\n",
    "    permutation_importance = PermutationImportance(clf, random_state=SEED)\n",
    "    permutation_importance.fit(X_val_scaled, \n",
    "                               train_df.loc[valid_idx, TARGET_COL])\n",
    "    if fold == 0:\n",
    "        df = eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50)\n",
    "    else:\n",
    "        df = pd.merge(df,eli5.explain_weights_df(permutation_importance, feature_names = FEATURES,top=50),on='feature')\n",
    "df.columns = ['feature','weight_1','std_1','weight_2','std_2','weight_3','std_3','weight_4','std_4','weight_5','std_5']\n",
    "df['feature_num'] = df.feature.apply(lambda x: colnames_dic2[x])\n",
    "df.index = df[['feature','feature_num']]\n",
    "s = df.drop([\"feature\",'feature_num'], axis=1).style.applymap(color_negative_red)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test_bed,pred11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_columns = [8,13,14,15,16,17,19,20,22,23,25,26,27,28,29,31,32,33,34,35,36]\n",
    "lgbm11 = LGBMRegressor(n_estimators = 500,random_state = 0, max_depth = 11, num_leaves = 20, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "pred11_new = lgbm11.fit(X_train_bed_scaled.drop(del_columns,axis=1), y_train_bed).predict(X_test_bed_scaled.drop(del_columns,axis=1))\n",
    "mape(y_test_bed,pred11_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8,13,14,15,16,17,19,20,22,23,25,26,27,28,29,31,32,33,34,35,36 번 피쳐 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 상품군별 삭제할 피쳐\n",
    "colnames_dic\n",
    "clothes_del_columns = [37,34,33,32,26,36,20,12,13,16,18]\n",
    "inner_del_columns = [34,33,32,26,36,37,28,17,18,9,27,12,13,25,22,8,28,14,19,15,16]\n",
    "kitchen_del_columns = [25,32,34,36,33,27,9,13,8,17,15,18,16,12,28,14,11]\n",
    "food_del_columns = [25,36,26,9,33,34,32,37,27,17,16,15,28,29,13,20,14]\n",
    "beauty_del_columns = [11,16,14,18,13,22,34,33,32,29,26,36,37,8,12,19,17,15]\n",
    "elec_del_columns = [8,10,11,12,13,14,17,18,19,25,32,33,34,35,36]\n",
    "goods_del_columns = [13,14,15,16,17,19,20,21,22,23,25,29,32,33,34,37]\n",
    "health_del_columns = [8,11,13,14,15,16,17,18,20,25,26,27,32,33,34,36,37]\n",
    "etc_del_columns = [9,11,13,14,15,16,17,18,19,20,22,23,25,27,32,33,34,35,36,37]\n",
    "furn_del_columns = [8,10,11,12,13,14,15,18,19,23,25,27,32,33,34,35,36]\n",
    "bed_del_columns = [8,13,14,15,16,17,19,20,22,23,25,26,27,28,29,31,32,33,34,35,36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm3 = LGBMRegressor(n_estimators = 1200,random_state = 0, max_depth = 11, learning_rate = 0.07, num_leaves = 23, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "lgbm6 = LGBMRegressor(n_estimators = 500,  num_leaves = 1024, boosting_type = \"dart\", random_state = 0, max_depth = 12, verbose = 0, n_jobs = -1, objective = 'mape')\n",
    "lgbm7 = LGBMRegressor(n_estimators = 600,random_state = 0, max_depth = 9, verbose = 0,  n_jobs = -1, objective = 'tweedie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "lgbm9 = LGBMRegressor(n_estimators = 1200, random_state = 0, boosting_type = 'dart', max_depth = 17, learning_rate = 0.15, verbose = 0, n_jobs = -1, objective = 'tweedie')\n",
    "\n",
    "\n",
    "pred9 = lgbm9.fit(X_train_etc_scaled, y_train_etc).predict(X_test_etc_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9, pred10, pred11]\n",
    "# predictions = [pred1_new, pred2_new, pred3_new, pred4_new, pred5_new, pred6_new, pred7_new, pred8_new, pred9_new, pred10_new, pred11_new] # selection한 결과물\n",
    "trues = [y_test_clothes, y_test_inner, y_test_kitchen, y_test_food, y_test_beauty, y_test_elec, y_test_goods, y_test_health, y_test_etc, y_test_furn, y_test_bed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피쳐추가이전\n",
    "mape_list = []\n",
    "for pred, true in zip(predictions, trues):\n",
    "    mape_res = mape(true, pred)\n",
    "    print(f\"mape: {mape_res}\" )\n",
    "    mape_list.append(mape_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_selected\n",
    "mape_list = []\n",
    "for pred, true in zip(predictions, trues):\n",
    "    mape_res = mape(true, pred)\n",
    "    print(f\"mape: {mape_res}\" )\n",
    "    mape_list.append(mape_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존 format으로 변경 후 mape 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros_like(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test format으로 다시 넣어주기\n",
    "for i in range(len(prod_group)):\n",
    "      y_pred[X_test[\"상품군\"] == prod_group[i]] = predictions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_tot_mape)\n",
    "# 수정전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_tot_mape)\n",
    "# 최종"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
