{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/merged_data_concat_0926.pickle', 'rb') as f:\n",
    "    performance_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_data['마더코드'] = performance_data['마더코드'].map(str)\n",
    "performance_data['상품코드'] = performance_data['상품코드'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **성능 Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **라벨인코딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코딩\n",
    "test_data = performance_data.drop(['방송일시','판매량', 'holiday', '대비', 'date', 'mean_rating', \n",
    "                                   '배당수익률(%)', '주가자산비율', '고가지수', '저가지수', '거래량(천주)', '거래대금(백만원)','상장시가총액(백만원)'],axis=1)\n",
    "\n",
    "# test_data['상품코드'] = test_data['상품코드'].map(int)\n",
    "for feat in ['상품명','상품코드','마더코드','prime_time','중분류','요일','season','남여','muil','브랜드','season_prod','소분류']:\n",
    "    lbe = LabelEncoder()\n",
    "    test_data[feat] = lbe.fit_transform(test_data[feat].astype(str).values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_group = test_data[\"상품군\"].unique()\n",
    "prod_group_dct = {v:k for k, v in enumerate(prod_group)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추후 전체 set에 대한 mape를 구하기 위해서 split을 해줌.\n",
    "\n",
    "predict_data = test_data[test_data.취급액 == -1].reset_index(drop=True)\n",
    "train_set = test_data[test_data['취급액'] != -1]\n",
    "\n",
    "X = train_set.drop([\"취급액\"], axis = 1)\n",
    "y = train_set[\"취급액\"]\n",
    "\n",
    "grp_idx = train_set['상품군'].map(prod_group_dct)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify = grp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = [len(X_test[X_test[\"상품군\"] == prod_group[i]]) for i in range(len(prod_group))]\n",
    "print(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold용 데이터 만들기\n",
    "def make_grp_data(X,idx):\n",
    "    X_data = X[X[\"상품군\"] == prod_group[idx]].drop([\"상품군\"], axis=1)\n",
    "#     y_data = y[X[\"상품군\"] == prod_group[idx]]['취급액']\n",
    "    return X_data\n",
    "\n",
    "clothes= make_grp_data(train_set,0)\n",
    "inner = make_grp_data(train_set,1)\n",
    "kitchen = make_grp_data(train_set,2)\n",
    "food = make_grp_data(train_set,3)\n",
    "beauty = make_grp_data(train_set,4)\n",
    "elec = make_grp_data(train_set,5)\n",
    "goods = make_grp_data(train_set,6)\n",
    "health = make_grp_data(train_set,7)\n",
    "etc = make_grp_data(train_set,8)\n",
    "furn = make_grp_data(train_set,9)\n",
    "bed = make_grp_data(train_set,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    result = np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_colnames = list(clothes.drop('취급액',axis=1).columns)\n",
    "new_colnames = []\n",
    "for i in range(len(clothes.drop('취급액',axis=1).columns)):\n",
    "    new_colnames.append(i)\n",
    "colnames_dic = {}\n",
    "for i in range(len(original_colnames)):\n",
    "    colnames_dic[str(new_colnames[i])] = original_colnames[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_colnames = list(X_train.columns)\n",
    "new_colnames = []\n",
    "for i in range(len(X_train.columns)):\n",
    "    new_colnames.append(i)\n",
    "colnames_dic2 = {}\n",
    "for i in range(len(original_colnames)):\n",
    "    colnames_dic2[str(new_colnames[i])] = original_colnames[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 상품군별 삭제할 피쳐\n",
    "all_del_colmns = [33,10,37,35,16,18,9,14,15,21,24,17]\n",
    "clothes_del_columns = [37,34,33,32,26,36,20,12,13,16,18]\n",
    "inner_del_columns = [34,33,32,26,36,37,28,17,18,9,27,12,13,25,22,8,28,14,19,15,16]\n",
    "kitchen_del_columns = [25,32,34,36,33,27,9,13,8,17,15,18,16,12,28,14,11]\n",
    "food_del_columns = [25,36,26,9,33,34,32,37,27,17,16,15,28,29,13,20,14]\n",
    "beauty_del_columns = [11,16,14,18,13,22,34,33,32,29,26,36,37,8,12,19,17,15]\n",
    "elec_del_columns = [8,10,11,12,13,14,17,18,19,25,32,33,34,35,36]\n",
    "goods_del_columns = [13,14,15,16,17,19,20,21,22,23,25,29,32,33,34,37]\n",
    "health_del_columns = [8,11,13,14,15,16,17,18,20,25,26,27,32,33,34,36,37]\n",
    "etc_del_columns = [9,11,13,14,15,16,17,18,19,20,22,23,25,27,32,33,34,35,36,37]\n",
    "furn_del_columns = [8,10,11,12,13,14,15,18,19,23,25,27,32,33,34,35,36]\n",
    "bed_del_columns = [8,13,14,15,16,17,19,20,22,23,25,26,27,28,29,31,32,33,34,35,36]\n",
    "\n",
    "all_del_colmns = [colnames_dic2[str(i)] for i in all_del_colmns]\n",
    "clothes_del_columns = [colnames_dic[str(i)] for i in clothes_del_columns]\n",
    "inner_del_columns = [colnames_dic[str(i)] for i in inner_del_columns]\n",
    "kitchen_del_columns = [colnames_dic[str(i)] for i in kitchen_del_columns]\n",
    "food_del_columns = [colnames_dic[str(i)] for i in food_del_columns]\n",
    "beauty_del_columns = [colnames_dic[str(i)] for i in beauty_del_columns]\n",
    "elec_del_columns = [colnames_dic[str(i)] for i in elec_del_columns]\n",
    "goods_del_columns = [colnames_dic[str(i)] for i in goods_del_columns]\n",
    "health_del_columns = [colnames_dic[str(i)] for i in health_del_columns]\n",
    "etc_del_columns = [colnames_dic[str(i)] for i in etc_del_columns]\n",
    "furn_del_columns = [colnames_dic[str(i)] for i in furn_del_columns]\n",
    "bed_del_columns = [colnames_dic[str(i)] for i in bed_del_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.drop(all_del_colmns,axis=1)\n",
    "\n",
    "clothes = clothes.drop(clothes_del_columns,axis=1)\n",
    "inner = inner.drop(inner_del_columns,axis=1)\n",
    "kitchen = kitchen.drop(kitchen_del_columns,axis=1)\n",
    "food = food.drop(food_del_columns,axis=1)\n",
    "beauty = beauty.drop(beauty_del_columns,axis=1)\n",
    "elec = elec.drop(elec_del_columns,axis=1)\n",
    "goods = goods.drop(goods_del_columns,axis=1)\n",
    "health = health.drop(health_del_columns,axis=1)\n",
    "etc = etc.drop(etc_del_columns,axis=1)\n",
    "furn = furn.drop(furn_del_columns,axis=1)\n",
    "bed = bed.drop(bed_del_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split된 걸 상품군 별로 나눔\n",
    "\n",
    "def train_test_grp(X_train, X_test, y_train, y_test, prod_group, grp_index):\n",
    "    new_X_train = X_train[X_train[\"상품군\"] == prod_group[grp_index]].drop(\"상품군\", axis=1)\n",
    "    new_X_test = X_test[X_test[\"상품군\"] == prod_group[grp_index]].drop(\"상품군\", axis=1)\n",
    "    new_y_train = y_train[X_train[\"상품군\"] == prod_group[grp_index]]\n",
    "    new_y_test = y_test[X_test[\"상품군\"] == prod_group[grp_index]]\n",
    "    return new_X_train, new_X_test, new_y_train, new_y_test\n",
    "\n",
    "X_train_clothes, X_test_clothes, y_train_clothes, y_test_clothes = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 0)\n",
    "X_train_inner, X_test_inner, y_train_inner, y_test_inner = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 1)\n",
    "X_train_kitchen, X_test_kitchen, y_train_kitchen, y_test_kitchen = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 2)\n",
    "X_train_food, X_test_food, y_train_food, y_test_food = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 3)\n",
    "X_train_beauty, X_test_beauty, y_train_beauty, y_test_beauty = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 4)\n",
    "X_train_elec, X_test_elec, y_train_elec, y_test_elec = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 5)\n",
    "X_train_goods, X_test_goods, y_train_goods, y_test_goods = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 6)\n",
    "X_train_health, X_test_health, y_train_health, y_test_health = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 7)\n",
    "X_train_etc, X_test_etc, y_train_etc, y_test_etc = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 8)\n",
    "X_train_furn, X_test_furn, y_train_furn, y_test_furn = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 9)\n",
    "X_train_bed, X_test_bed, y_train_bed, y_test_bed = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_clothes = X_train_clothes.drop(clothes_del_columns,axis=1)\n",
    "X_train_inner = X_train_inner.drop(inner_del_columns,axis=1)\n",
    "X_train_kitchen = X_train_kitchen.drop(kitchen_del_columns,axis=1)\n",
    "X_train_food = X_train_food.drop(food_del_columns,axis=1)\n",
    "X_train_beauty = X_train_beauty.drop(beauty_del_columns,axis=1)\n",
    "X_train_elec = X_train_elec.drop(elec_del_columns,axis=1)\n",
    "X_train_goods = X_train_goods.drop(goods_del_columns,axis=1)\n",
    "X_train_health = X_train_health.drop(health_del_columns,axis=1)\n",
    "X_train_etc = X_train_etc.drop(etc_del_columns,axis=1)\n",
    "X_train_furn = X_train_furn.drop(furn_del_columns,axis=1)\n",
    "X_train_bed = X_train_bed.drop(bed_del_columns,axis=1)\n",
    "\n",
    "\n",
    "X_test_clothes = X_test_clothes.drop(clothes_del_columns,axis=1)\n",
    "X_test_inner = X_test_inner.drop(inner_del_columns,axis=1)\n",
    "X_test_kitchen = X_test_kitchen.drop(kitchen_del_columns,axis=1)\n",
    "X_test_food = X_test_food.drop(food_del_columns,axis=1)\n",
    "X_test_beauty = X_test_beauty.drop(beauty_del_columns,axis=1)\n",
    "X_test_elec = X_test_elec.drop(elec_del_columns,axis=1)\n",
    "X_test_goods = X_test_goods.drop(goods_del_columns,axis=1)\n",
    "X_test_health = X_test_health.drop(health_del_columns,axis=1)\n",
    "X_test_etc = X_test_etc.drop(etc_del_columns,axis=1)\n",
    "X_test_furn = X_test_furn.drop(furn_del_columns,axis=1)\n",
    "X_test_bed = X_test_bed.drop(bed_del_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['상품군'] = lbe.fit_transform(test_data['상품군'].astype(str).values)\n",
    "train_set = test_data[test_data['취급액'] != -1]\n",
    "\n",
    "X = train_set.drop([\"취급액\"], axis = 1)\n",
    "y = train_set[\"취급액\"]\n",
    "\n",
    "# grp_idx = train_set['상품군'].map(prod_group_dct)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify = grp_idx)\n",
    "\n",
    "X_train = X_train.drop(all_del_colmns,axis=1)\n",
    "X_test = X_test.drop(all_del_colmns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "X_train_clothes_scaled = pd.DataFrame(scaler.fit_transform(X_train_clothes))\n",
    "X_test_clothes_scaled = pd.DataFrame(scaler.transform(X_test_clothes))\n",
    "X_train_inner_scaled = pd.DataFrame(scaler.fit_transform(X_train_inner))\n",
    "X_test_inner_scaled = pd.DataFrame(scaler.transform(X_test_inner))\n",
    "X_train_kitchen_scaled = pd.DataFrame(scaler.fit_transform(X_train_kitchen))\n",
    "X_test_kitchen_scaled = pd.DataFrame(scaler.transform(X_test_kitchen))\n",
    "X_train_food_scaled = pd.DataFrame(scaler.fit_transform(X_train_food))\n",
    "X_test_food_scaled = pd.DataFrame(scaler.transform(X_test_food))\n",
    "X_train_beauty_scaled = pd.DataFrame(scaler.fit_transform(X_train_beauty))\n",
    "X_test_beauty_scaled = pd.DataFrame(scaler.transform(X_test_beauty))\n",
    "X_train_elec_scaled = pd.DataFrame(scaler.fit_transform(X_train_elec))\n",
    "X_test_elec_scaled = pd.DataFrame(scaler.transform(X_test_elec))\n",
    "X_train_goods_scaled = pd.DataFrame(scaler.fit_transform(X_train_goods))\n",
    "X_test_goods_scaled = pd.DataFrame(scaler.transform(X_test_goods))\n",
    "X_train_health_scaled = pd.DataFrame(scaler.fit_transform(X_train_health))\n",
    "X_test_health_scaled = pd.DataFrame(scaler.transform(X_test_health))\n",
    "X_train_etc_scaled = pd.DataFrame(scaler.fit_transform(X_train_etc))\n",
    "X_test_etc_scaled = pd.DataFrame(scaler.transform(X_test_etc))\n",
    "X_train_furn_scaled = pd.DataFrame(scaler.fit_transform(X_train_furn))\n",
    "X_test_furn_scaled = pd.DataFrame(scaler.transform(X_test_furn))\n",
    "X_train_bed_scaled = pd.DataFrame(scaler.fit_transform(X_train_bed))\n",
    "X_test_bed_scaled = pd.DataFrame(scaler.transform(X_test_bed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 22.1579455684485  \n",
    "mape: 26.5763001623147  \n",
    "mape: 27.49715254148381  \n",
    "mape: 15.874558341118911  \n",
    "mape: 19.217135630635212  \n",
    "mape: 40.07481282197277  \n",
    "mape: 31.623747738429696  \n",
    "mape: 21.80460073752754  \n",
    "mape: 38.29687137332689  \n",
    "mape: 41.68921261024886  \n",
    "mape: 25.20877367938055  \n",
    "\n",
    "최종: 29.200866247957723"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = train_set    \n",
    "    LGBM = LGBMRegressor(objective = 'gamma',\n",
    "        booster='dart',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "#         feature_fraction=feature_fraction,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "#         'feature_fraction': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2500),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "bo = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo.maximize(n_iter = 200, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_param(x):\n",
    "    best_params = x.max['params']\n",
    "    best_params['random_state'] = 0\n",
    "    if x == (bo_6 or bo_10):\n",
    "        best_params['objective'] = 'mape'\n",
    "    else:\n",
    "        best_params['objective'] = 'gamma'\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "    best_params['min_child_samples'] = int(best_params['min_child_samples'])\n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "    best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMRegressor(**make_param(bo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = clothes    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clothes\n",
    "bo_1 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_1.maximize(n_iter = 300, init_points = 20, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = inner    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 26.5763001623147  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner\n",
    "bo_2 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_2.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = kitchen    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 27.49715254148381  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kitchen\n",
    "bo_3 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_3.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = food    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 15.874558341118911  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# food\n",
    "bo_4 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_4.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = beauty    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "  \n",
    "mape: 19.217135630635212  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beauty\n",
    "bo_5 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_5.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = elec    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,               \n",
    "        random_state=0,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 40.07481282197277  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elec\n",
    "bo_6 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_6.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = goods    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 31.623747738429696  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goods\n",
    "bo_7 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_7.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = health    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 21.80460073752754  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# health\n",
    "bo_8 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_8.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = etc    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,                 \n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 38.29687137332689  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc\n",
    "bo_9 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_9.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = furn    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,               \n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "  \n",
    "mape: 41.68921261024886  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# furn\n",
    "bo_10 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_10.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth,learning_rate,min_split_gain,min_child_samples,\n",
    "             sub_sample,n_estimators,min_data_in_leaf,min_child_weight):\n",
    "\n",
    "    data = bed    \n",
    "    LGBM = LGBMRegressor(objective = 'tweedie',\n",
    "        num_leaves = int(num_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "        min_child_samples = int(min_child_samples),\n",
    "        min_data_in_leaf= int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,              \n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction= bagging_fraction,\n",
    "        learning_rate= learning_rate,\n",
    "        min_split_gain = min_split_gain,\n",
    "        sub_sample = sub_sample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        LGBM.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred[valid_idx] = LGBM.predict(valid_x, num_iteration = LGBM.best_iteration_)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'num_leaves': (12, 1024),\n",
    "        'feature_fraction': (0.1, 0.9),\n",
    "        'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'learning_rate': (0.05, 0.2),\n",
    "        'min_split_gain': (0.001, 0.1),\n",
    "        'min_child_samples': (0, 100),\n",
    "        'sub_sample': (0.7, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "        'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 25.20877367938055  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bed\n",
    "bo_11 = BayesianOptimization(lgb_eval, pbounds = params,random_state=0)\n",
    "bo_11.maximize(n_iter = 300, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_param(x):\n",
    "    best_params = x.max['params']\n",
    "    best_params['random_state'] = 0\n",
    "    if x == (bo_6 or bo_10):\n",
    "        best_params['objective'] = 'mape'\n",
    "    else:\n",
    "        best_params['objective'] = 'tweedie'\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "    best_params['min_child_samples'] = int(best_params['min_child_samples'])\n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "    best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_list = [bo_1,bo_2,bo_3,bo_4,bo_5,bo_6,bo_7,bo_8,bo_9,bo_10,bo_11]\n",
    "for i in range(len(bo_list)):\n",
    "    globals()['lgbm{}'.format(i+1)] = LGBMRegressor(**make_param(bo_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_1.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = train_set    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clothes\n",
    "bo = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo.maximize(n_iter = 100, init_points = 20, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_param_xgb(x):\n",
    "    best_params = x.max['params']\n",
    "    best_params['random_state'] = 0\n",
    "    if x == (bo_6 or bo_10):\n",
    "        best_params['objective'] = mape_objective_function\n",
    "    else:\n",
    "        best_params['objective'] = 'reg:tweedie'\n",
    "\n",
    "    \n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "#     best_params['min_child_samples'] = int(best_params['min_child_samples'])\n",
    "    best_params['max_leaves'] = int(best_params['max_leaves'])\n",
    "#     best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])\n",
    "    return best_params\n",
    "xgb_all = make_param_xgb(bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRegressor(**xgb_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = clothes    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        booster = 'dart',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_objective_function(labels,preds):\n",
    "    \n",
    "    grad = (preds - labels) / (0.2 + labels * np.abs(preds - labels))\n",
    "    hess = 0.1 + np.zeros(len(preds));\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clothes\n",
    "bo_1 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_1.maximize(n_iter = 100, init_points = 20, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = inner    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        booster = 'dart',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 26.5763001623147  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner\n",
    "bo_2 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_2.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = kitchen    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 27.49715254148381  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kitchen\n",
    "bo_3 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_3.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = food    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 15.874558341118911  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# food\n",
    "bo_4 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_4.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "    data = beauty    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "    \n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "  \n",
    "mape: 19.217135630635212  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beauty\n",
    "bo_5 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_5.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = elec    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 40.07481282197277  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elec\n",
    "bo_6 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_6.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = goods    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 31.623747738429696  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goods\n",
    "bo_7 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_7.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = health    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 21.80460073752754  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# health\n",
    "bo_8 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_8.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "    data = etc    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 38.29687137332689  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc\n",
    "bo_9 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_9.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = furn    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "  \n",
    "mape: 41.68921261024886  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# furn\n",
    "bo_10 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_10.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval(max_leaves,colsample_bytree,max_depth,eta,gamma,\n",
    "             subsample,n_estimators,min_child_weight):\n",
    "\n",
    "    data = bed    \n",
    "    xgb = XGBRegressor(objective = 'reg:tweedie',\n",
    "        max_leaves  = int(max_leaves),\n",
    "        max_depth = int(max_depth),\n",
    "#         min_child_samples = int(min_child_samples),\n",
    "#         min_child_weight = int(min_data_in_leaf),\n",
    "        n_jobs=6,\n",
    "        random_state=0,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "#         bagging_fraction= bagging_fraction,\n",
    "        eta= eta,\n",
    "        gamma = gamma,\n",
    "        subsample = subsample,\n",
    "        n_estimators= int(n_estimators),\n",
    "        min_child_weight= min_child_weight\n",
    "    )\n",
    "\n",
    "    folds = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "        \n",
    "    test_pred = np.zeros(data.shape[0])\n",
    "    \n",
    "    feats = [f for f in data.columns if f != '취급액']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(data[feats], data['취급액'])):\n",
    "        train_x, train_y = data[feats].iloc[train_idx], data['취급액'].iloc[train_idx]\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        valid_x, valid_y = data[feats].iloc[valid_idx], data['취급액'].iloc[valid_idx]\n",
    "        valid_x = scaler.transform(valid_x)\n",
    "        xgb.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)],\n",
    "                verbose=False,early_stopping_rounds = 100)\n",
    "        \n",
    "        best_iteration = xgb.get_booster().best_ntree_limit\n",
    "        test_pred[valid_idx] = xgb.predict(valid_x, ntree_limit = best_iteration)\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "   \n",
    "  \n",
    "    return -mape(data['취급액'],test_pred)\n",
    "\n",
    "params = {'max_leaves': (12, 1024),\n",
    "        'colsample_bytree': (0.1, 0.9),\n",
    "#         'bagging_fraction': (0.5, 1),\n",
    "        'max_depth': (7, 20),\n",
    "        'eta': (0.05, 0.2),\n",
    "        'gamma': (0.001, 0.1),\n",
    "#         'min_child_samples': (0, 100),\n",
    "        'subsample': (0.5, 1),\n",
    "        'n_estimators': (500, 2000),\n",
    "#         'min_data_in_leaf': (0, 1000),\n",
    "        'min_child_weight': (0.001, 1000)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    " \n",
    "mape: 25.20877367938055  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clothes, inner, kitchen, food, beauty, elec, goods, health, etc, furn, bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bed\n",
    "bo_11 = BayesianOptimization(xgb_eval, pbounds = params,random_state=0)\n",
    "bo_11.maximize(n_iter = 100, init_points = 10, acq = 'ei', xi = 0.01, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_param_xgb(x):\n",
    "    best_params = x.max['params']\n",
    "    best_params['random_state'] = 0\n",
    "    if x == (bo_6 or bo_10):\n",
    "        best_params['objective'] = mape_objective_function\n",
    "    else:\n",
    "        best_params['objective'] = 'reg:tweedie'\n",
    "    best_params['max_leaves'] = best_params.pop('num_leaves')\n",
    "    best_params['colsample_bytree'] = best_params.pop('feature_fraction')\n",
    "    best_params['eta'] = best_params.pop('learning_rate')\n",
    "    best_params['gamma'] = best_params.pop('min_split_gain')\n",
    "    best_params['subsample'] = best_params.pop('sub_sample')\n",
    "    \n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "#     best_params['min_child_samples'] = int(best_params['min_child_samples'])\n",
    "    best_params['max_leaves'] = int(best_params['max_leaves'])\n",
    "#     best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_list = [bo_1,bo_2,bo_3,bo_4,bo_5,bo_6,bo_7,bo_8,bo_9,bo_10,bo_11]\n",
    "for i in range(len(bo_list)):\n",
    "    globals()['xgb{}'.format(i+1)] = XGBRegressor(**make_param_xgb(bo_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CATBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split된 걸 상품군 별로 나눔\n",
    "\n",
    "def train_test_grp(X_train, X_test, y_train, y_test, prod_group, grp_index):\n",
    "    new_X_train = X_train[X_train[\"상품군\"] == prod_group[grp_index]].drop(\"상품군\", axis=1)\n",
    "    new_X_test = X_test[X_test[\"상품군\"] == prod_group[grp_index]].drop(\"상품군\", axis=1)\n",
    "    new_y_train = y_train[X_train[\"상품군\"] == prod_group[grp_index]]\n",
    "    new_y_test = y_test[X_test[\"상품군\"] == prod_group[grp_index]]\n",
    "    return new_X_train, new_X_test, new_y_train, new_y_test\n",
    "\n",
    "X_train_clothes_cat, X_test_clothes_cat, y_train_clothes_cat, y_test_clothes_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 0)\n",
    "X_train_inner_cat, X_test_inner_cat, y_train_inner_cat, y_test_inner_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 1)\n",
    "X_train_kitchen_cat, X_test_kitchen_cat, y_train_kitchen_cat, y_test_kitchen_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 2)\n",
    "X_train_food_cat, X_test_food_cat, y_train_food_cat, y_test_food_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 3)\n",
    "X_train_beauty_cat, X_test_beauty_cat, y_train_beauty_cat, y_test_beauty_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 4)\n",
    "X_train_elec_cat, X_test_elec_cat, y_train_elec_cat, y_test_elec_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 5)\n",
    "X_train_goods_cat, X_test_goods_cat, y_train_goods_cat, y_test_goods_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 6)\n",
    "X_train_health_cat, X_test_health_cat, y_train_health_cat, y_test_health_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 7)\n",
    "X_train_etc_cat, X_test_etc_cat, y_train_etc_cat, y_test_etc_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 8)\n",
    "X_train_furn_cat, X_test_furn_cat, y_train_furn_cat, y_test_furn_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 9)\n",
    "X_train_bed_cat, X_test_bed_cat, y_train_bed_cat, y_test_bed_cat = train_test_grp(X_train, X_test, y_train, y_test, prod_group, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **clothes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "25.145399979198285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_clothes = CatBoostRegressor(n_estimators = 3000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=1,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clothes_cat = CatBoost_clothes.fit(X_train_clothes_cat, y_train_clothes_cat).predict(X_test_clothes_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(pred_clothes_cat,y_test_clothes_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "24.23689010234943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_clothes = CatBoostRegressor(n_estimators = 3200,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 8,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_clothes_cat = CatBoost_clothes.fit(X_train_clothes_cat, y_train_clothes_cat).predict(X_test_clothes_cat)\n",
    "print(mape(pred_clothes_cat,y_test_clothes_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **inner**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "43.99056799986272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_inner = CatBoostRegressor(n_estimators = 3000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_inner_cat = CatBoost_inner.fit(X_train_inner_cat, y_train_inner_cat).predict(X_test_inner_cat)\n",
    "print(mape(pred_inner_cat,y_test_inner_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "29.53169226635896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_inner = CatBoostRegressor(n_estimators = 4000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_inner_cat = CatBoost_inner.fit(X_train_inner_cat, y_train_inner_cat).predict(X_test_inner_cat)\n",
    "print(mape(pred_inner_cat,y_test_inner_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **kitchen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "64.55274205390863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_kitchen = CatBoostRegressor(n_estimators = 3000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_kitchen_cat = CatBoost_kitchen.fit(X_train_kitchen_cat, y_train_kitchen_cat).predict(X_test_kitchen_cat)\n",
    "print(mape(pred_kitchen_cat,y_test_kitchen_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "30.24951438451378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_kitchen = CatBoostRegressor(n_estimators = 4000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_kitchen_cat = CatBoost_kitchen.fit(X_train_kitchen_cat, y_train_kitchen_cat).predict(X_test_kitchen_cat)\n",
    "print(mape(pred_kitchen_cat,y_test_kitchen_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **food**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "19.704997926203458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_food = CatBoostRegressor(n_estimators = 4000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_food_cat = CatBoost_food.fit(X_train_food_cat, y_train_food_cat).predict(X_test_food_cat)\n",
    "print(mape(pred_food_cat,y_test_food_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "17.998711705582476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_food = CatBoostRegressor(n_estimators = 6000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 9,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_food_cat = CatBoost_food.fit(X_train_food_cat, y_train_food_cat).predict(X_test_food_cat)\n",
    "print(mape(pred_food_cat,y_test_food_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **beauty**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "18.052581659186103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_beauty = CatBoostRegressor(n_estimators = 4000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_beauty_cat = CatBoost_beauty.fit(X_train_beauty_cat, y_train_beauty_cat).predict(X_test_beauty_cat)\n",
    "print(mape(pred_beauty_cat,y_test_beauty_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "18.052581659186103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_beauty = CatBoostRegressor(n_estimators = 4000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_beauty_cat = CatBoost_beauty.fit(X_train_beauty_cat, y_train_beauty_cat).predict(X_test_beauty_cat)\n",
    "print(mape(pred_beauty_cat,y_test_beauty_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **elec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "42.00516138344872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_elec = CatBoostRegressor(n_estimators = 4000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_elec_cat = CatBoost_elec.fit(X_train_elec_cat, y_train_elec_cat).predict(X_test_elec_cat)\n",
    "print(mape(pred_elec_cat,y_test_elec_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "40.749478469398454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_elec = CatBoostRegressor(n_estimators = 6000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 9,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_elec_cat = CatBoost_elec.fit(X_train_elec_cat, y_train_elec_cat).predict(X_test_elec_cat)\n",
    "print(mape(pred_elec_cat,y_test_elec_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **goods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "31.892540437625026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_goods = CatBoostRegressor(n_estimators = 4500,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 8,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_goods_cat = CatBoost_goods.fit(X_train_goods_cat, y_train_goods_cat).predict(X_test_goods_cat)\n",
    "print(mape(pred_goods_cat,y_test_goods_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "31.70004831869703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_goods = CatBoostRegressor(n_estimators = 5500,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 8,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_goods_cat = CatBoost_goods.fit(X_train_goods_cat, y_train_goods_cat).predict(X_test_goods_cat)\n",
    "print(mape(pred_goods_cat,y_test_goods_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **health**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "25.5302415747131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_health = CatBoostRegressor(n_estimators = 4000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_health_cat = CatBoost_health.fit(X_train_health_cat, y_train_health_cat).predict(X_test_health_cat)\n",
    "print(mape(pred_health_cat,y_test_health_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "25.513938134667402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_health = CatBoostRegressor(n_estimators = 5200,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_health_cat = CatBoost_health.fit(X_train_health_cat, y_train_health_cat).predict(X_test_health_cat)\n",
    "print(mape(pred_health_cat,y_test_health_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **etc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "32.83289173591722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_etc = CatBoostRegressor(n_estimators = 3500,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_etc_cat = CatBoost_etc.fit(X_train_etc_cat, y_train_etc_cat).predict(X_test_etc_cat)\n",
    "print(mape(pred_etc_cat,y_test_etc_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "32.378940364044254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_etc = CatBoostRegressor(n_estimators = 5000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 8,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_etc_cat = CatBoost_etc.fit(X_train_etc_cat, y_train_etc_cat).predict(X_test_etc_cat)\n",
    "print(mape(pred_etc_cat,y_test_etc_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **furn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "41.95334916764361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_furn = CatBoostRegressor(n_estimators = 3500,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_furn_cat = CatBoost_furn.fit(X_train_furn_cat, y_train_furn_cat).predict(X_test_furn_cat)\n",
    "print(mape(pred_furn_cat,y_test_furn_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "41.95317552001951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_furn = CatBoostRegressor(n_estimators = 4000,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_furn_cat = CatBoost_furn.fit(X_train_furn_cat, y_train_furn_cat).predict(X_test_furn_cat)\n",
    "print(mape(pred_furn_cat,y_test_furn_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **bed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 전**  \n",
    "25.25815984598247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_bed = CatBoostRegressor(n_estimators = 3500,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_bed_cat = CatBoost_bed.fit(X_train_bed_cat, y_train_bed_cat).predict(X_test_bed_cat)\n",
    "print(mape(pred_bed_cat,y_test_bed_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **튜닝 후**  \n",
    "25.180334318321613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_bed = CatBoostRegressor(n_estimators = 4500,\n",
    "                             loss_function = 'MAE',\n",
    "#                              loss_function = 'Tweedie:variance_power=1.1',\n",
    "                             eval_metric = 'MAPE',\n",
    "#                              learning_rate = 0.1,\n",
    "                             depth = 7,\n",
    "                             cat_features = ['상품코드','마더코드','상품명','소분류','prime_time','중분류','요일','season','남여','muil','season_prod','season','브랜드'],                             \n",
    "                             verbose=0,\n",
    "                             random_seed = 0,\n",
    "                             task_type='CPU')\n",
    "pred_bed_cat = CatBoost_bed.fit(X_train_bed_cat, y_train_bed_cat).predict(X_test_bed_cat)\n",
    "print(mape(pred_bed_cat,y_test_bed_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_clothes.save_model(\"../model/boosting_model/catboost_clothes.cbm\")\n",
    "CatBoost_inner.save_model(\"../model/boosting_model/catboost_inner.cbm\")\n",
    "CatBoost_kitchen.save_model(\"../model/boosting_model/catboost_kitchen.cbm\")\n",
    "CatBoost_food.save_model(\"../model/boosting_model/catboost_food.cbm\")\n",
    "CatBoost_beauty.save_model(\"../model/boosting_model/catboost_beauty.cbm\")\n",
    "CatBoost_elec.save_model(\"../model/boosting_model/catboost_elec.cbm\")\n",
    "CatBoost_goods.save_model(\"../model/boosting_model/catboost_goods.cbm\")\n",
    "CatBoost_health.save_model(\"../model/boosting_model/catboost_health.cbm\")\n",
    "CatBoost_etc.save_model(\"../model/boosting_model/catboost_etc.cbm\")\n",
    "CatBoost_furn.save_model(\"../model/boosting_model/catboost_furn.cbm\")\n",
    "CatBoost_bed.save_model(\"../model/boosting_model/catboost_bed.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_clothes = CatBoostRegressor()\n",
    "CatBoost_inner = CatBoostRegressor()\n",
    "CatBoost_kitchen = CatBoostRegressor()\n",
    "CatBoost_food = CatBoostRegressor()\n",
    "CatBoost_beauty = CatBoostRegressor()\n",
    "CatBoost_elec = CatBoostRegressor()\n",
    "CatBoost_goods = CatBoostRegressor()\n",
    "CatBoost_health = CatBoostRegressor()\n",
    "CatBoost_etc = CatBoostRegressor()\n",
    "CatBoost_furn = CatBoostRegressor()\n",
    "CatBoost_bed = CatBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_clothes.load_model(\"../model/boosting_model/catboost_clothes.cbm\")\n",
    "CatBoost_inner.load_model(\"../model/boosting_model/catboost_inner.cbm\")\n",
    "CatBoost_kitchen.load_model(\"../model/boosting_model/catboost_kitchen.cbm\")\n",
    "CatBoost_food.load_model(\"../model/boosting_model/catboost_food.cbm\")\n",
    "CatBoost_beauty.load_model(\"../model/boosting_model/catboost_beauty.cbm\")\n",
    "CatBoost_elec.load_model(\"../model/boosting_model/catboost_elec.cbm\")\n",
    "CatBoost_goods.load_model(\"../model/boosting_model/catboost_goods.cbm\")\n",
    "CatBoost_health.load_model(\"../model/boosting_model/catboost_health.cbm\")\n",
    "CatBoost_etc.load_model(\"../model/boosting_model/catboost_etc.cbm\")\n",
    "CatBoost_furn.load_model(\"../model/boosting_model/catboost_furn.cbm\")\n",
    "CatBoost_bed.load_model(\"../model/boosting_model/catboost_bed.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **튜닝 성능 확인**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgbm6 = LGBMRegressor(n_estimators = 500,  num_leaves = 2048, boosting_type = \"dart\", random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'mape')\n",
    "\n",
    "lgbm10 = LGBMRegressor(n_estimators = 500, boosting_type = 'dart', num_leaves = 127, random_state = 0, max_depth = 11, verbose = 0, n_jobs = -1, objective = 'mape', learning_rate = 0.09)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clothes_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "\n",
    "pred1 = lgbm1.fit(X_train_clothes_scaled, y_train_clothes).predict(X_test_clothes_scaled)\n",
    "pred2 = lgbm2.fit(X_train_inner_scaled, y_train_inner).predict(X_test_inner_scaled)\n",
    "pred3 = lgbm3.fit(X_train_kitchen_scaled, y_train_kitchen).predict(X_test_kitchen_scaled)\n",
    "pred4 = lgbm4.fit(X_train_food_scaled, y_train_food).predict(X_test_food_scaled)\n",
    "pred5 = lgbm5.fit(X_train_beauty_scaled, y_train_beauty).predict(X_test_beauty_scaled)\n",
    "pred6 = lgbm6.fit(X_train_elec_scaled, y_train_elec).predict(X_test_elec_scaled)\n",
    "pred7 = lgbm7.fit(X_train_goods_scaled, y_train_goods).predict(X_test_goods_scaled)\n",
    "pred8 = lgbm8.fit(X_train_health_scaled, y_train_health).predict(X_test_health_scaled)\n",
    "pred9 = lgbm9.fit(X_train_etc_scaled, y_train_etc).predict(X_test_etc_scaled)\n",
    "pred10 = lgbm10.fit(X_train_furn_scaled, y_train_furn).predict(X_test_furn_scaled)\n",
    "pred11 = lgbm11.fit(X_train_bed_scaled, y_train_bed).predict(X_test_bed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9, pred10, pred11] # selection한 결과물\n",
    "trues = [y_test_clothes, y_test_inner, y_test_kitchen, y_test_food, y_test_beauty, y_test_elec, y_test_goods, y_test_health, y_test_etc, y_test_furn, y_test_bed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 22.1579455684485  \n",
    "mape: 26.5763001623147  \n",
    "mape: 27.49715254148381  \n",
    "mape: 15.874558341118911  \n",
    "mape: 19.217135630635212  \n",
    "mape: 40.07481282197277  \n",
    "mape: 31.623747738429696  \n",
    "mape: 21.80460073752754  \n",
    "mape: 38.29687137332689  \n",
    "mape: 41.68921261024886  \n",
    "mape: 25.20877367938055  \n",
    "\n",
    "최종: 29.200866247957723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LGBM 튜닝 후\n",
    "mape_list = []\n",
    "for pred, true in zip(predictions, trues):\n",
    "    mape_res = mape(true, pred)\n",
    "    print(f\"mape: {mape_res}\" )\n",
    "    mape_list.append(mape_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존 format으로 변경 후 mape 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgbm = np.zeros_like(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test format으로 다시 넣어주기\n",
    "for i in range(len(prod_group)):\n",
    "      y_pred_lgbm[X_test[\"상품군\"] == prod_group[i]] = predictions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Save\n",
    "def model_save(today, best_tot_mape):\n",
    "    import joblib\n",
    "\n",
    "    joblib.dump(lgbm1, f\"../model/boosting_model/lgbm1.pickle\")\n",
    "    joblib.dump(lgbm2, f\"../model/boosting_model/lgbm2.pickle\")\n",
    "    joblib.dump(lgbm3, f\"../model/boosting_model/lgbm3.pickle\")\n",
    "    joblib.dump(lgbm4, f\"../model/boosting_model/lgbm4.pickle\")\n",
    "    joblib.dump(lgbm5, f\"../model/boosting_model/lgbm5.pickle\")\n",
    "    joblib.dump(lgbm6, f\"../model/boosting_model/lgbm6.pickle\")\n",
    "    joblib.dump(lgbm7, f\"../model/boosting_model/lgbm7.pickle\")\n",
    "    joblib.dump(lgbm8, f\"../model/boosting_model/lgbm8.pickle\")\n",
    "    joblib.dump(lgbm9, f\"../model/boosting_model/lgbm9.pickle\")\n",
    "    joblib.dump(lgbm10, f\"../model/boosting_model/lgbm10.pickle\")\n",
    "    joblib.dump(lgbm11, f\"../model/boosting_model/lgbm11.pickle\")\n",
    "\n",
    "  ## result 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_mape = mape(y_test, y_pred_lgbm)\n",
    "\n",
    "if tot_mape < best_tot_mape:\n",
    "    best_tot_mape = tot_mape\n",
    "    model_save(today, best_tot_mape)\n",
    "\n",
    "\n",
    "print(f\"MAPE calculated over total valid set is {tot_mape:.4f}\\nThe current best mape score is {best_tot_mape:2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb6 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.6224866603723187,\n",
    "             eta=0.08799374038096733, gamma=0.016737988780906453, gpu_id=-1,\n",
    "             importance_type='gain', interaction_constraints='',\n",
    "             learning_rate=0.087993741, max_delta_step=0, max_depth=13,\n",
    "             max_leaves=676, min_child_weight=244.42634757601076, \n",
    "             monotone_constraints='()', n_estimators=665, n_jobs=0,\n",
    "             num_parallel_tree=1,\n",
    "             objective='reg:tweedie',\n",
    "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "             subsample=0.7414548854045842, tree_method='exact',\n",
    "             validate_parameters=1, verbosity=None)\n",
    "\n",
    "xgb10 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.789956577441571,\n",
    "             eta=0.1409391962108311, gamma=0.06058367377353676, gpu_id=-1,\n",
    "             importance_type='gain', interaction_constraints='',\n",
    "             learning_rate=0.140939191, max_delta_step=0, max_depth=10,\n",
    "             max_leaves=279, min_child_weight=568.9477690044143,\n",
    "             monotone_constraints='()', n_estimators=1399, n_jobs=0,\n",
    "             num_parallel_tree=1, objective='reg:tweedie', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
    "             subsample=0.7017180830644052, tree_method='exact',\n",
    "             validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "\n",
    "pred1 = xgb1.fit(X_train_clothes_scaled, y_train_clothes).predict(X_test_clothes_scaled)\n",
    "pred2 = xgb2.fit(X_train_inner_scaled, y_train_inner).predict(X_test_inner_scaled)\n",
    "pred3 = xgb3.fit(X_train_kitchen_scaled, y_train_kitchen).predict(X_test_kitchen_scaled)\n",
    "pred4 = xgb4.fit(X_train_food_scaled, y_train_food).predict(X_test_food_scaled)\n",
    "pred5 = xgb5.fit(X_train_beauty_scaled, y_train_beauty).predict(X_test_beauty_scaled)\n",
    "pred6 = xgb6.fit(X_train_elec_scaled, y_train_elec).predict(X_test_elec_scaled)\n",
    "pred7 = xgb7.fit(X_train_goods_scaled, y_train_goods).predict(X_test_goods_scaled)\n",
    "pred8 = xgb8.fit(X_train_health_scaled, y_train_health).predict(X_test_health_scaled)\n",
    "pred9 = xgb9.fit(X_train_etc_scaled, y_train_etc).predict(X_test_etc_scaled)\n",
    "pred10 = xgb10.fit(X_train_furn_scaled, y_train_furn).predict(X_test_furn_scaled)\n",
    "pred11 = xgb11.fit(X_train_bed_scaled, y_train_bed).predict(X_test_bed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9, pred10, pred11] # selection한 결과물\n",
    "trues = [y_test_clothes, y_test_inner, y_test_kitchen, y_test_food, y_test_beauty, y_test_elec, y_test_goods, y_test_health, y_test_etc, y_test_furn, y_test_bed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **튜닝 전 성능**\n",
    "\n",
    "mape: 22.1579455684485  \n",
    "mape: 26.5763001623147  \n",
    "mape: 27.49715254148381  \n",
    "mape: 15.874558341118911  \n",
    "mape: 19.217135630635212  \n",
    "mape: 40.07481282197277  \n",
    "mape: 31.623747738429696  \n",
    "mape: 21.80460073752754  \n",
    "mape: 38.29687137332689  \n",
    "mape: 41.68921261024886  \n",
    "mape: 25.20877367938055  \n",
    "\n",
    "최종: 29.200866247957723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LGBM 튜닝 후\n",
    "mape_list = []\n",
    "for pred, true in zip(predictions, trues):\n",
    "    mape_res = mape(true, pred)\n",
    "    print(f\"mape: {mape_res}\" )\n",
    "    mape_list.append(mape_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGB 튜닝 후\n",
    "mape_list = []\n",
    "for pred, true in zip(predictions, trues):\n",
    "    mape_res = mape(true, pred)\n",
    "    print(f\"mape: {mape_res}\" )\n",
    "    mape_list.append(mape_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존 format으로 변경 후 mape 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros_like(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test format으로 다시 넣어주기\n",
    "for i in range(len(prod_group)):\n",
    "      y_pred[X_test[\"상품군\"] == prod_group[i]] = predictions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Save\n",
    "def model_save(today, best_tot_mape):\n",
    "    import joblib\n",
    "\n",
    "    joblib.dump(xgb1, f\"../model/boosting_model/xgb1.pickle\")\n",
    "    joblib.dump(xgb2, f\"../model/boosting_model/xgb2.pickle\")\n",
    "    joblib.dump(xgb3, f\"../model/boosting_model/xgb3.pickle\")\n",
    "    joblib.dump(xgb4, f\"../model/boosting_model/xgb4.pickle\")\n",
    "    joblib.dump(xgb5, f\"../model/boosting_model/xgb5.pickle\")\n",
    "    joblib.dump(xgb6, f\"../model/boosting_model/xgb6.pickle\")\n",
    "    joblib.dump(xgb7, f\"../model/boosting_model/xgb7.pickle\")\n",
    "    joblib.dump(xgb8, f\"../model/boosting_model/xgb8.pickle\")\n",
    "    joblib.dump(xgb9, f\"../model/boosting_model/xgb9.pickle\")\n",
    "    joblib.dump(xgb10, f\"../model/boosting_model/xgb10.pickle\")\n",
    "    joblib.dump(xgb11, f\"../model/boosting_model/xgb11.pickle\")\n",
    "\n",
    "  ## result 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_mape = mape(y_test, y_pred)\n",
    "\n",
    "if tot_mape < best_tot_mape:\n",
    "    best_tot_mape = tot_mape\n",
    "    model_save(today, best_tot_mape)\n",
    "\n",
    "\n",
    "print(f\"MAPE calculated over total valid set is {tot_mape:.4f}\\nThe current best mape score is {best_tot_mape:2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
