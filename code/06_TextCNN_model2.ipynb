{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRu5ntHc31Cv"
   },
   "source": [
    "# Text CNN\n",
    "상품명 정보 등을 활용한 입력값으로 하는 Text CNN 모델.\n",
    "TextCNN 뒤에 dense layer가 여러개 추가되는 형태.\n",
    "\n",
    "1) Input  \n",
    ": 시간 정보, 마더 코드, 상품군, 상품명.\n",
    "상품명 하나를 집어넣기보다는 시간 정보 등을 넣어줌. \n",
    "word embedding의 값은 표준정규분포를 따르도록 초기화됨.\n",
    "\n",
    "2) Convolutions and MaxPooling  \n",
    ": Convolution 연산을 통해서 텍스트의 특징과 패턴을 추출함. 이후 MaxPooling을 통해서 가장 뚜렷한 특징을 추출.\n",
    "\n",
    "3) FC Layer  \n",
    ": FC layer 4개를 통과하여 추출해낸 특징의 분포를 다양하게 탐색하며, 취급액의 분포를 근사."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHqJmh-GIqWA"
   },
   "source": [
    "## Contents\n",
    "\n",
    "- Data 준비  \n",
    "- Dataset 구축    \n",
    "- Model 정의  \n",
    "- Model Training  \n",
    "- Model Evaluation  \n",
    "- Predict Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gElfkLyvJanD"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8HIemTMir3n8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SmiYYxveJZaJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BQ1A3TfgWk0O"
   },
   "outputs": [],
   "source": [
    "from torchtext import data as td\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "E7DVqvcE6ngX",
    "outputId": "cffaacf2-38bf-4410-8f7a-dcf1a171a46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-28\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qz3Rw_jYr3oC",
    "outputId": "f178001d-c47d-44b6-94e0-03fb83e36784"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7d176c96d8>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0) # random seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XVtW2VqEKgl1",
    "outputId": "dc762607-5ccb-4ac4-c2d1-2cefe0b51f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v3qId_KBKjKF"
   },
   "outputs": [],
   "source": [
    "data_path = \"../data/\" # 데이터 경로\n",
    "output_path = \"../data\" # prediction 저장 경로\n",
    "model_path = \"../data\" # 모델 저장 경로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opxL7OvYNzh6"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nJYrosdhQSa6"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, \"df_0927_yujin.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ptb-sVM3Mj8k",
    "outputId": "ee2487ab-6bb7-4a48-daf0-7435c655ae19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape: (38095, 54)\n"
     ]
    }
   ],
   "source": [
    "print(f\"DataFrame Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv9yZXcnQOrj"
   },
   "source": [
    "## Data 준비\n",
    ": 월, 요일, 시간, 마더코드, 상품군, 소분류, 상품명을 하나의 string으로 묶어서 input으로 준비  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RDOTwusGQtLb"
   },
   "outputs": [],
   "source": [
    "df[\"마더코드\"] = df[\"마더코드\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "pJU9Bulgx1EH",
    "outputId": "b2cec043-fa90-4514-9fda-acf6ededca07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              01월 06시 00분 화요일 100346 의류 니트 테이트 남성 셀린니트 3종\n",
       "1              01월 06시 00분 화요일 100346 의류 니트 테이트 여성 셀린니트 3종\n",
       "2              01월 06시 20분 화요일 100346 의류 니트 테이트 남성 셀린니트 3종\n",
       "3              01월 06시 20분 화요일 100346 의류 니트 테이트 여성 셀린니트 3종\n",
       "4              01월 06시 40분 화요일 100346 의류 니트 테이트 남성 셀린니트 3종\n",
       "                               ...                        \n",
       "38090    07월 00시 10분 수요일 100099 속옷 드로즈 일시불 라쉬반 fc바르셀로나 ...\n",
       "38091    07월 00시 10분 수요일 100099 속옷 드로즈 무이자 라쉬반 fc바르셀로나 ...\n",
       "38092     07월 00시 10분 수요일 100099 속옷 드로즈 라쉬반 fc바르셀로나 드로즈 8종\n",
       "38093       07월 01시 20분 수요일 100261 의류 티셔츠 아놀드파마 티셔츠 레깅스 세트\n",
       "38094       07월 01시 40분 수요일 100261 의류 티셔츠 아놀드파마 티셔츠 레깅스 세트\n",
       "Length: 38095, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df[\"방송일시\"].str.replace(\"-\",\"\").apply(lambda x: x[4:-3]).str.replace(\":\", \" \").apply(lambda x: x[:2] + \"월\" + x[4:7] + \"시\" + x[7:] + \"분\")\n",
    "text_time = tmp + \" \" + df[\"요일\"] + \" \" + df[\"마더코드\"] + \" \" + df[\"상품군\"] + \" \" + df[\"소분류\"] + \" \" + df[\"상품명\"]\n",
    "text_time = text_time.str.strip()\n",
    "\n",
    "text_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG9WVZTQSDvL"
   },
   "source": [
    "### token length 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "MfQPaHHGRAoB",
    "outputId": "795c3b59-fbbc-46f7-c76d-4f45e2239a9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.9670e+03, 9.3360e+03, 1.0529e+04, 7.5190e+03, 5.3120e+03,\n",
       "        1.8240e+03, 4.4400e+02, 1.0700e+02, 6.0000e+00, 5.1000e+01]),\n",
       " array([ 9. , 10.1, 11.2, 12.3, 13.4, 14.5, 15.6, 16.7, 17.8, 18.9, 20. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = text_time.str.split(\" \")\n",
    "token_length = tokens.apply(lambda x: len(x))\n",
    "plt.hist(token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VWQKdP3S6_T"
   },
   "source": [
    "## Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "35pRtVYaTkHF"
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.concat([text_time, df[[\"상품군\",\"취급액\"]]], axis = 1)\n",
    "df_tmp.columns = [\"상품정보\", \"상품군\", \"취급액\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8pvQXkXMUNwj"
   },
   "outputs": [],
   "source": [
    "train_data = df_tmp[df_tmp[\"취급액\"] != -1]\n",
    "test_data = df_tmp[df_tmp[\"취급액\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4MVPDC_vRHsN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "grp_dct = {v:k for k, v in enumerate(train_data[\"상품군\"].unique())}\n",
    "grp_idx = train_data[\"상품군\"].map(grp_dct)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data[\"상품정보\"],train_data['취급액'].astype(float), random_state = 0, stratify = grp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "mhw7pOD8Ssxz",
    "outputId": "ba76a304-6d16-49a8-d946-23cbec2e279b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (26534,)\n",
      "X_test shape: (8845,)\n",
      "y_train shape: (26534,)\n",
      "y_test shape: (8845,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_val.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aM5zWDSSRN6C"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([X_train, y_train]).T.to_csv(os.path.join(data_path, \"text_train.csv\"), index = False)\n",
    "pd.DataFrame([X_val, y_val]).T.to_csv(os.path.join(data_path, \"text_valid.csv\"), index = False)\n",
    "train_data[[\"상품정보\", \"취급액\"]].to_csv(os.path.join(data_path, \"text_whole_train.csv\"), index = False)\n",
    "test_data[[\"상품정보\", \"취급액\"]].to_csv(os.path.join(data_path, \"text_test.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1-L7ifZr3qJ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "LMYJpClX8qik",
    "outputId": "782c37b4-bee9-440f-93ec-b4b2eee8c915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Batch Size: 16\n",
      "Model Learning Rate: 0.003\n",
      "Model Number Of Epochs: 100\n",
      "Model Weight Decay Rate: 1e-05\n",
      "Model Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model Parameter\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.003\n",
    "N_EPOCHS = 100\n",
    "WEIGHT_DECAY = 1e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Model Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Model Learning Rate: {LR}\")\n",
    "print(f\"Model Number Of Epochs: {N_EPOCHS}\")\n",
    "print(f\"Model Weight Decay Rate: {WEIGHT_DECAY}\")\n",
    "print(f\"Model Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSD3K6bxr3qK"
   },
   "source": [
    "### Dataset 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gxVnlVK4r3qZ"
   },
   "outputs": [],
   "source": [
    "# field 정의\n",
    "\n",
    "PRODUCT = td.Field(sequential = True, \n",
    "                   use_vocab = True, \n",
    "                   batch_first = True)\n",
    "\n",
    "SALES = td.Field(sequential = False,\n",
    "                 use_vocab = False,\n",
    "                 preprocessing = lambda x: float(x),\n",
    "                 dtype = torch.float,\n",
    "                 batch_first = True)\n",
    "\n",
    "fields = [('text', PRODUCT), ('target', SALES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OQUiFKblr3qe"
   },
   "outputs": [],
   "source": [
    "# data 불러오기\n",
    "\n",
    "train_data,val_data = td.TabularDataset.splits(\n",
    "                        path = data_path,\n",
    "                        train = \"text_train.csv\",\n",
    "                        validation = \"text_valid.csv\",                       \n",
    "                        format = \"csv\",\n",
    "                        fields = fields,\n",
    "                        skip_header = True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NzTAXA12-oUK",
    "outputId": "22967fbc-b7e5-4111-e538-72279f3bda26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2976\n"
     ]
    }
   ],
   "source": [
    "PRODUCT.build_vocab(train_data)\n",
    "print(f\"Vocabulary Size: {len(PRODUCT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GaqVtHW6r3qx"
   },
   "outputs": [],
   "source": [
    "# Batch iterator\n",
    "\n",
    "train_iterator, valid_iterator = td.BucketIterator.splits(\n",
    "(train_data, val_data),\n",
    "    sort = False,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e7JT_vUXKwu"
   },
   "source": [
    "### 모델 아키텍처 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HdT9pv-Br3rA"
   },
   "outputs": [],
   "source": [
    "# code reference: https://github.com/kh-kim/simple-ntc/blob/master/simple_ntc/cnn.py\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    - Input\n",
    "    1) input_size: Vocabulary Size (int)\n",
    "    2) word_vec_dim: Embedding Dimension (int)\n",
    "    3) dropout_p: Dropout Rate (float)\n",
    "    4) window_sizes: A list of window sizes (list)\n",
    "    5) n_filters: A list of which the number of filters for each window size (list)\n",
    "    6) hidden_sizes: hidden layer sizes (list)\n",
    "\n",
    "    window size means that how many words a single pattern covers,\n",
    "    and n_filter means that how many patterns to cover.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 word_vec_dim,\n",
    "                 dropout_p = .5,\n",
    "                 window_sizes = [3,4,5],\n",
    "                 n_filters = [100,100,100],\n",
    "                 hidden_sizes = [1]\n",
    "                ):\n",
    "        self.input_size = input_size\n",
    "        self.word_vec_dim = word_vec_dim\n",
    "        self.dropout_p = dropout_p\n",
    "        self.window_sizes = window_sizes\n",
    "        self.n_filters = n_filters\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(input_size, word_vec_dim)\n",
    "        \n",
    "        # convolution layer 개수는 window_sizes 개수에 따라 달라지므로, layer를 더하기 위해 setattr, getattr 메소드를 사용할 것\n",
    "        # 이 layer가 sequential하게 이어지는 게 아니라 각각 독립적인 convolution block.\n",
    "        # layer 개수는 window_sizes*n_filters의 가중합.\n",
    "        # setattr: 속성 할당 ex) setattr(obj, attr_name, attr_value)\n",
    "        # getattr: 속성 반환 ex) getattr(obj, attr_name, value to be returned when attr doesnt exist(option))\n",
    "        \n",
    "        for window_size, n_filter in zip(window_sizes, n_filters): # convolution layers\n",
    "            cnn = nn.Conv2d(in_channels = 1, \n",
    "                           out_channels = n_filter, \n",
    "                           kernel_size = (window_size, word_vec_dim)\n",
    "                           )\n",
    "            \n",
    "            setattr(self, \"cnn-%d-%d\" % (window_size, n_filter), cnn)\n",
    "\n",
    "        for idx in range(len(hidden_sizes)): # hidden layers\n",
    "            if idx == 0:\n",
    "              dim0 = sum(n_filters)\n",
    "            else:\n",
    "              dim0 = hidden_sizes[idx-1]\n",
    "            \n",
    "            affine = nn.Linear(dim0, hidden_sizes[idx])\n",
    "            \n",
    "            setattr(self, \"linear-%d-%d\" % (dim0, hidden_sizes[idx]), affine)\n",
    "\n",
    "            \n",
    "        self.relu = nn.ReLU() # activation layer\n",
    "        self.dropout = nn.Dropout(dropout_p) # dropout\n",
    "        self.generator = nn.Linear(hidden_sizes[idx], 1) # output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input -> (embedding) -> Convolution (window_sizes * n_filters) -> ReLU -> Dropout -> max_pooling1D -> Dense -> Output\n",
    "\n",
    "        - Tracking the changing size\n",
    "        1) input: |x| = (batch_size, length)\n",
    "        2) After Embedding: |x| = (batch_size, length, word_vec_dim)\n",
    "        3) After Convolution: |cnn_out| = (batch_size, n_filter, length - window_size + 1, 1)\n",
    "        4) After MaxPooling: |cnn_out| = (batch_size, n_filters)\n",
    "        5) After Concat: |cnn_outs| = (batch_size, sum(n_filters))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Embedding Layers\n",
    "        x = self.emb(x)\n",
    "        min_length = max(self.window_sizes)\n",
    "        \n",
    "        ## Window sizes보다 Sequence Length의 길이가 작을 때를 대비하여 zero padding\n",
    "        if min_length > x.size(1): \n",
    "            pad = x.new(x.size(0), min_length - x.size(1), self.word_vec_dim).zero_()\n",
    "            x = torch.cat([x, pad], dim = 1) \n",
    "      \n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Conv Layers\n",
    "        cnn_outs = []\n",
    "        for window_size, n_filter in zip(self.window_sizes, self.n_filters):\n",
    "            cnn = getattr(self, \"cnn-%d-%d\" % (window_size, n_filter))\n",
    "            cnn_out = self.dropout(self.relu(cnn(x)))\n",
    "          \n",
    "            # MaxPooling Layers          \n",
    "            cnn_out = nn.functional.max_pool1d(input = cnn_out.squeeze(-1),\n",
    "                                              kernel_size = cnn_out.size(-2)\n",
    "                                              ).squeeze(-1)\n",
    "            \n",
    "            cnn_outs += [cnn_out] # len(cnn_out) for each: number of feature map\n",
    "        \n",
    "        cnn_outs = torch.cat(cnn_outs, dim = -1) # hstack\n",
    "        \n",
    "        # Dense Layers\n",
    "        affine_in = cnn_outs\n",
    "        for idx in range(len(self.hidden_sizes)):\n",
    "          if idx == 0:\n",
    "            dim0 = sum(self.n_filters)\n",
    "          else:\n",
    "            dim0 = self.hidden_sizes[idx-1]\n",
    "          affine = getattr(self, \"linear-%d-%d\" % (dim0, self.hidden_sizes[idx]))\n",
    "          affine_out = self.dropout(self.relu(affine(affine_in)))\n",
    "          affine_in = affine_out\n",
    "\n",
    "        # Output\n",
    "        y = self.generator(affine_out)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4p1FMisr3rH"
   },
   "source": [
    "### Model Train\n",
    ": MAPE 정의, train & evaluate 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7cxFSftOr3rW"
   },
   "outputs": [],
   "source": [
    "def mape(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Loss Function & Eval metric at the same time.\n",
    "    Returns mape\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs((y_true-y_pred)/y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4JkudzH4r3rb"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    model training\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_mape = 0\n",
    "    \n",
    "    model.train() # train\n",
    "    \n",
    "    for batch in iterator:\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "        loss_ = mape(predictions, batch.target)\n",
    "        mape_ = mape(predictions, batch.target)\n",
    "        \n",
    "        loss_.backward() # backpropagation\n",
    "        optimizer.step() # updating parameters\n",
    "        \n",
    "        epoch_loss += loss_.item()\n",
    "        epoch_mape += mape_.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_mape / len(iterator)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6e0bzbvfr3rg"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    \"\"\"\n",
    "    evaluating model\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_mape = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in iterator:\n",
    "            \n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss_ = mape(predictions, batch.target)\n",
    "            mape_ = mape(predictions, batch.target)\n",
    "    \n",
    "            \n",
    "            epoch_loss += loss_.item()\n",
    "            epoch_mape += mape_.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_mape / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bH2a7D6Wr3rn"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    \"\"\"\n",
    "    check epoch time\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-OP1PVzEr3qP"
   },
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "\"\"\"\n",
    "ReduceLROnPlateau: loss가 더이상 감소하지 않으면 learning rate를 조정함.\n",
    "factor: learning rate decay rate\n",
    "patience: after steps of patience, learning rate decaying started\n",
    "\"\"\"\n",
    "model = TextCNN(len(PRODUCT.vocab), \n",
    "                32,\n",
    "                0.1,\n",
    "                window_sizes = [1,2,3,4,5,6,7,8,9],\n",
    "                n_filters = [15,15,30,35,35,30,15,15,15],\n",
    "                hidden_sizes = [64,16,4])\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR, weight_decay = WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=10, verbose=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2KCGPs_ar3ru",
    "outputId": "457c43a8-be56-4fc5-faa5-40c40a4f8f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 92.197 | Train MAPE: 92.20%\n",
      "\t Val. Loss: 60.593 |  Val. MAPE: 60.59%\n",
      "Epoch: 02 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 54.436 | Train MAPE: 54.44%\n",
      "\t Val. Loss: 49.716 |  Val. MAPE: 49.72%\n",
      "Epoch: 03 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 48.678 | Train MAPE: 48.68%\n",
      "\t Val. Loss: 46.708 |  Val. MAPE: 46.71%\n",
      "Epoch: 04 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 46.212 | Train MAPE: 46.21%\n",
      "\t Val. Loss: 45.514 |  Val. MAPE: 45.51%\n",
      "Epoch: 05 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 44.613 | Train MAPE: 44.61%\n",
      "\t Val. Loss: 44.608 |  Val. MAPE: 44.61%\n",
      "Epoch: 06 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 43.419 | Train MAPE: 43.42%\n",
      "\t Val. Loss: 43.155 |  Val. MAPE: 43.15%\n",
      "Epoch: 07 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 42.468 | Train MAPE: 42.47%\n",
      "\t Val. Loss: 41.893 |  Val. MAPE: 41.89%\n",
      "Epoch: 08 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 41.139 | Train MAPE: 41.14%\n",
      "\t Val. Loss: 42.027 |  Val. MAPE: 42.03%\n",
      "Epoch: 09 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 39.882 | Train MAPE: 39.88%\n",
      "\t Val. Loss: 39.596 |  Val. MAPE: 39.60%\n",
      "Epoch: 10 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 39.197 | Train MAPE: 39.20%\n",
      "\t Val. Loss: 38.035 |  Val. MAPE: 38.04%\n",
      "Epoch: 11 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 38.310 | Train MAPE: 38.31%\n",
      "\t Val. Loss: 38.306 |  Val. MAPE: 38.31%\n",
      "Epoch: 12 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 37.294 | Train MAPE: 37.29%\n",
      "\t Val. Loss: 37.343 |  Val. MAPE: 37.34%\n",
      "Epoch: 13 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 36.600 | Train MAPE: 36.60%\n",
      "\t Val. Loss: 37.010 |  Val. MAPE: 37.01%\n",
      "Epoch: 14 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 36.227 | Train MAPE: 36.23%\n",
      "\t Val. Loss: 35.423 |  Val. MAPE: 35.42%\n",
      "Epoch: 15 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 35.643 | Train MAPE: 35.64%\n",
      "\t Val. Loss: 37.835 |  Val. MAPE: 37.84%\n",
      "Epoch: 16 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 35.328 | Train MAPE: 35.33%\n",
      "\t Val. Loss: 35.304 |  Val. MAPE: 35.30%\n",
      "Epoch: 17 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 35.083 | Train MAPE: 35.08%\n",
      "\t Val. Loss: 34.979 |  Val. MAPE: 34.98%\n",
      "Epoch: 18 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 34.595 | Train MAPE: 34.60%\n",
      "\t Val. Loss: 35.428 |  Val. MAPE: 35.43%\n",
      "Epoch: 19 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 34.693 | Train MAPE: 34.69%\n",
      "\t Val. Loss: 34.134 |  Val. MAPE: 34.13%\n",
      "Epoch: 20 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 34.133 | Train MAPE: 34.13%\n",
      "\t Val. Loss: 35.401 |  Val. MAPE: 35.40%\n",
      "Epoch: 21 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 33.840 | Train MAPE: 33.84%\n",
      "\t Val. Loss: 33.833 |  Val. MAPE: 33.83%\n",
      "Epoch: 22 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 33.670 | Train MAPE: 33.67%\n",
      "\t Val. Loss: 33.789 |  Val. MAPE: 33.79%\n",
      "Epoch: 23 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 33.417 | Train MAPE: 33.42%\n",
      "\t Val. Loss: 34.251 |  Val. MAPE: 34.25%\n",
      "Epoch: 24 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 33.368 | Train MAPE: 33.37%\n",
      "\t Val. Loss: 35.742 |  Val. MAPE: 35.74%\n",
      "Epoch: 25 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 33.070 | Train MAPE: 33.07%\n",
      "\t Val. Loss: 34.500 |  Val. MAPE: 34.50%\n",
      "Epoch: 26 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 32.817 | Train MAPE: 32.82%\n",
      "\t Val. Loss: 33.883 |  Val. MAPE: 33.88%\n",
      "Epoch: 27 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 32.702 | Train MAPE: 32.70%\n",
      "\t Val. Loss: 34.973 |  Val. MAPE: 34.97%\n",
      "Epoch: 28 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 32.763 | Train MAPE: 32.76%\n",
      "\t Val. Loss: 34.377 |  Val. MAPE: 34.38%\n",
      "Epoch: 29 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 32.445 | Train MAPE: 32.45%\n",
      "\t Val. Loss: 32.615 |  Val. MAPE: 32.61%\n",
      "Epoch: 30 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 32.536 | Train MAPE: 32.54%\n",
      "\t Val. Loss: 34.293 |  Val. MAPE: 34.29%\n",
      "Epoch: 31 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 32.270 | Train MAPE: 32.27%\n",
      "\t Val. Loss: 34.018 |  Val. MAPE: 34.02%\n",
      "Epoch: 32 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 32.207 | Train MAPE: 32.21%\n",
      "\t Val. Loss: 33.494 |  Val. MAPE: 33.49%\n",
      "Epoch: 33 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 32.099 | Train MAPE: 32.10%\n",
      "\t Val. Loss: 33.269 |  Val. MAPE: 33.27%\n",
      "Epoch: 34 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.913 | Train MAPE: 31.91%\n",
      "\t Val. Loss: 33.763 |  Val. MAPE: 33.76%\n",
      "Epoch: 35 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.881 | Train MAPE: 31.88%\n",
      "\t Val. Loss: 34.224 |  Val. MAPE: 34.22%\n",
      "Epoch: 36 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.799 | Train MAPE: 31.80%\n",
      "\t Val. Loss: 33.431 |  Val. MAPE: 33.43%\n",
      "Epoch: 37 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.600 | Train MAPE: 31.60%\n",
      "\t Val. Loss: 33.328 |  Val. MAPE: 33.33%\n",
      "Epoch: 38 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.531 | Train MAPE: 31.53%\n",
      "\t Val. Loss: 33.405 |  Val. MAPE: 33.41%\n",
      "Epoch: 39 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.430 | Train MAPE: 31.43%\n",
      "\t Val. Loss: 32.230 |  Val. MAPE: 32.23%\n",
      "Epoch: 40 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.291 | Train MAPE: 31.29%\n",
      "\t Val. Loss: 33.616 |  Val. MAPE: 33.62%\n",
      "Epoch: 41 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.258 | Train MAPE: 31.26%\n",
      "\t Val. Loss: 33.268 |  Val. MAPE: 33.27%\n",
      "Epoch: 42 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 31.284 | Train MAPE: 31.28%\n",
      "\t Val. Loss: 34.409 |  Val. MAPE: 34.41%\n",
      "Epoch: 43 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 31.070 | Train MAPE: 31.07%\n",
      "\t Val. Loss: 32.537 |  Val. MAPE: 32.54%\n",
      "Epoch: 44 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 30.923 | Train MAPE: 30.92%\n",
      "\t Val. Loss: 33.394 |  Val. MAPE: 33.39%\n",
      "Epoch: 45 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.734 | Train MAPE: 30.73%\n",
      "\t Val. Loss: 33.194 |  Val. MAPE: 33.19%\n",
      "Epoch: 46 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.747 | Train MAPE: 30.75%\n",
      "\t Val. Loss: 32.311 |  Val. MAPE: 32.31%\n",
      "Epoch: 47 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.767 | Train MAPE: 30.77%\n",
      "\t Val. Loss: 32.904 |  Val. MAPE: 32.90%\n",
      "Epoch: 48 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.759 | Train MAPE: 30.76%\n",
      "\t Val. Loss: 33.916 |  Val. MAPE: 33.92%\n",
      "Epoch: 49 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.492 | Train MAPE: 30.49%\n",
      "\t Val. Loss: 32.887 |  Val. MAPE: 32.89%\n",
      "Epoch    50: reducing learning rate of group 0 to 2.4000e-03.\n",
      "Epoch: 50 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.455 | Train MAPE: 30.46%\n",
      "\t Val. Loss: 32.757 |  Val. MAPE: 32.76%\n",
      "Epoch: 51 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.300 | Train MAPE: 30.30%\n",
      "\t Val. Loss: 33.336 |  Val. MAPE: 33.34%\n",
      "Epoch: 52 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.006 | Train MAPE: 30.01%\n",
      "\t Val. Loss: 33.876 |  Val. MAPE: 33.88%\n",
      "Epoch: 53 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 30.169 | Train MAPE: 30.17%\n",
      "\t Val. Loss: 32.271 |  Val. MAPE: 32.27%\n",
      "Epoch: 54 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.832 | Train MAPE: 29.83%\n",
      "\t Val. Loss: 32.513 |  Val. MAPE: 32.51%\n",
      "Epoch: 55 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.905 | Train MAPE: 29.90%\n",
      "\t Val. Loss: 33.749 |  Val. MAPE: 33.75%\n",
      "Epoch: 56 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 30.066 | Train MAPE: 30.07%\n",
      "\t Val. Loss: 33.467 |  Val. MAPE: 33.47%\n",
      "Epoch: 57 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.669 | Train MAPE: 29.67%\n",
      "\t Val. Loss: 32.711 |  Val. MAPE: 32.71%\n",
      "Epoch: 58 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.512 | Train MAPE: 29.51%\n",
      "\t Val. Loss: 32.409 |  Val. MAPE: 32.41%\n",
      "Epoch: 59 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.592 | Train MAPE: 29.59%\n",
      "\t Val. Loss: 33.982 |  Val. MAPE: 33.98%\n",
      "Epoch: 60 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.619 | Train MAPE: 29.62%\n",
      "\t Val. Loss: 34.142 |  Val. MAPE: 34.14%\n",
      "Epoch    61: reducing learning rate of group 0 to 1.9200e-03.\n",
      "Epoch: 61 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 29.403 | Train MAPE: 29.40%\n",
      "\t Val. Loss: 32.355 |  Val. MAPE: 32.36%\n",
      "Epoch: 62 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.360 | Train MAPE: 29.36%\n",
      "\t Val. Loss: 32.830 |  Val. MAPE: 32.83%\n",
      "Epoch: 63 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.226 | Train MAPE: 29.23%\n",
      "\t Val. Loss: 33.466 |  Val. MAPE: 33.47%\n",
      "Epoch: 64 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 28.903 | Train MAPE: 28.90%\n",
      "\t Val. Loss: 32.606 |  Val. MAPE: 32.61%\n",
      "Epoch: 65 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.161 | Train MAPE: 29.16%\n",
      "\t Val. Loss: 32.193 |  Val. MAPE: 32.19%\n",
      "Epoch: 66 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.909 | Train MAPE: 28.91%\n",
      "\t Val. Loss: 33.074 |  Val. MAPE: 33.07%\n",
      "Epoch: 67 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 29.007 | Train MAPE: 29.01%\n",
      "\t Val. Loss: 33.154 |  Val. MAPE: 33.15%\n",
      "Epoch: 68 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.857 | Train MAPE: 28.86%\n",
      "\t Val. Loss: 32.857 |  Val. MAPE: 32.86%\n",
      "Epoch: 69 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.740 | Train MAPE: 28.74%\n",
      "\t Val. Loss: 33.328 |  Val. MAPE: 33.33%\n",
      "Epoch: 70 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 28.734 | Train MAPE: 28.73%\n",
      "\t Val. Loss: 32.352 |  Val. MAPE: 32.35%\n",
      "Epoch: 71 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.506 | Train MAPE: 28.51%\n",
      "\t Val. Loss: 32.212 |  Val. MAPE: 32.21%\n",
      "Epoch: 72 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.726 | Train MAPE: 28.73%\n",
      "\t Val. Loss: 32.205 |  Val. MAPE: 32.21%\n",
      "Epoch: 73 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 28.425 | Train MAPE: 28.43%\n",
      "\t Val. Loss: 31.796 |  Val. MAPE: 31.80%\n",
      "Epoch: 74 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.704 | Train MAPE: 28.70%\n",
      "\t Val. Loss: 32.322 |  Val. MAPE: 32.32%\n",
      "Epoch: 75 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.593 | Train MAPE: 28.59%\n",
      "\t Val. Loss: 32.077 |  Val. MAPE: 32.08%\n",
      "Epoch: 76 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.566 | Train MAPE: 28.57%\n",
      "\t Val. Loss: 32.178 |  Val. MAPE: 32.18%\n",
      "Epoch: 77 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.518 | Train MAPE: 28.52%\n",
      "\t Val. Loss: 32.850 |  Val. MAPE: 32.85%\n",
      "Epoch: 78 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.394 | Train MAPE: 28.39%\n",
      "\t Val. Loss: 32.127 |  Val. MAPE: 32.13%\n",
      "Epoch: 79 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.523 | Train MAPE: 28.52%\n",
      "\t Val. Loss: 32.594 |  Val. MAPE: 32.59%\n",
      "Epoch: 80 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.389 | Train MAPE: 28.39%\n",
      "\t Val. Loss: 32.382 |  Val. MAPE: 32.38%\n",
      "Epoch: 81 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.290 | Train MAPE: 28.29%\n",
      "\t Val. Loss: 32.382 |  Val. MAPE: 32.38%\n",
      "Epoch: 82 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.235 | Train MAPE: 28.23%\n",
      "\t Val. Loss: 32.391 |  Val. MAPE: 32.39%\n",
      "Epoch: 83 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.378 | Train MAPE: 28.38%\n",
      "\t Val. Loss: 32.022 |  Val. MAPE: 32.02%\n",
      "Epoch    84: reducing learning rate of group 0 to 1.5360e-03.\n",
      "Epoch: 84 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.231 | Train MAPE: 28.23%\n",
      "\t Val. Loss: 32.294 |  Val. MAPE: 32.29%\n",
      "Epoch: 85 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 27.935 | Train MAPE: 27.94%\n",
      "\t Val. Loss: 32.539 |  Val. MAPE: 32.54%\n",
      "Epoch: 86 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 28.015 | Train MAPE: 28.02%\n",
      "\t Val. Loss: 31.979 |  Val. MAPE: 31.98%\n",
      "Epoch: 87 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.979 | Train MAPE: 27.98%\n",
      "\t Val. Loss: 32.257 |  Val. MAPE: 32.26%\n",
      "Epoch: 88 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.793 | Train MAPE: 27.79%\n",
      "\t Val. Loss: 32.420 |  Val. MAPE: 32.42%\n",
      "Epoch: 89 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.866 | Train MAPE: 27.87%\n",
      "\t Val. Loss: 31.930 |  Val. MAPE: 31.93%\n",
      "Epoch: 90 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 27.763 | Train MAPE: 27.76%\n",
      "\t Val. Loss: 31.931 |  Val. MAPE: 31.93%\n",
      "Epoch: 91 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 27.846 | Train MAPE: 27.85%\n",
      "\t Val. Loss: 32.339 |  Val. MAPE: 32.34%\n",
      "Epoch: 92 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.680 | Train MAPE: 27.68%\n",
      "\t Val. Loss: 31.888 |  Val. MAPE: 31.89%\n",
      "Epoch: 93 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.516 | Train MAPE: 27.52%\n",
      "\t Val. Loss: 32.456 |  Val. MAPE: 32.46%\n",
      "Epoch: 94 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.717 | Train MAPE: 27.72%\n",
      "\t Val. Loss: 32.540 |  Val. MAPE: 32.54%\n",
      "Epoch    95: reducing learning rate of group 0 to 1.2288e-03.\n",
      "Epoch: 95 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.714 | Train MAPE: 27.71%\n",
      "\t Val. Loss: 31.870 |  Val. MAPE: 31.87%\n",
      "Epoch: 96 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.389 | Train MAPE: 27.39%\n",
      "\t Val. Loss: 31.844 |  Val. MAPE: 31.84%\n",
      "Epoch: 97 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.308 | Train MAPE: 27.31%\n",
      "\t Val. Loss: 31.485 |  Val. MAPE: 31.49%\n",
      "Epoch: 98 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.355 | Train MAPE: 27.36%\n",
      "\t Val. Loss: 31.761 |  Val. MAPE: 31.76%\n",
      "Epoch: 99 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.317 | Train MAPE: 27.32%\n",
      "\t Val. Loss: 31.718 |  Val. MAPE: 31.72%\n",
      "Epoch: 100 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 27.271 | Train MAPE: 27.27%\n",
      "\t Val. Loss: 32.049 |  Val. MAPE: 32.05%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "save_checkpoint = False # whether to save checkpoint\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_mape = train(model, train_iterator, optimizer)\n",
    "    valid_loss, valid_mape = evaluate(model, valid_iterator)\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "      best_valid_loss = valid_loss\n",
    "      \n",
    "      ## save the checkpoint\n",
    "      if save_checkpoint:\n",
    "        torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': valid_loss\n",
    "              }, f\"{model_path}{today}-model_checkpoint.tar\")\n",
    "      else: ## save the model only\n",
    "        torch.save(model.state_dict(), f\"{model_path}{today}-model.pt\")\n",
    "      \n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train MAPE: {train_mape:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. MAPE: {valid_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "8eDbxAgCOeVk",
    "outputId": "792a2f19-6b7b-4665-ffbd-209f01b82604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best valid loss for 31.485136621899457 epochs : 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"best valid loss for {best_valid_loss} epochs : {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "bx-Nv8PIzz6W",
    "outputId": "a6937f83-2a72-42ed-9117-c966fd9e8b04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7cba9f5780>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Loss Plot\")\n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(valid_loss_list)\n",
    "plt.legend([\"train_loss\", \"validation_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5TyLs-2ocLz"
   },
   "source": [
    "### 전체 Train set으로 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_oCZz7ZV1PFZ"
   },
   "outputs": [],
   "source": [
    "final_train_data = td.TabularDataset.splits(\n",
    "                        path = data_path,\n",
    "                        train = \"text_whole_train.csv\",\n",
    "                        #test = \"text_test.csv\",\n",
    "                        format = \"csv\",\n",
    "                        fields = fields,\n",
    "                        skip_header = True\n",
    "                      )[0] # tuple unpacking\n",
    "\n",
    "PRODUCT.build_vocab(final_train_data)\n",
    "whole_train_iterator = td.BucketIterator(final_train_data, batch_size = BATCH_SIZE, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "h5kTpbJjAgSW",
    "outputId": "c4e17bed-eb88-4645-fa89-95f993292842"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2986"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PRODUCT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EObXJTxookWm"
   },
   "outputs": [],
   "source": [
    "# 전체 트레인셋에 대해 학습 후 최종 예측 진행\n",
    "\n",
    "def final_train(train_iterator, n_epochs):\n",
    "  model = TextCNN(len(PRODUCT.vocab), \n",
    "                32,\n",
    "                0.1,\n",
    "                window_sizes = [1,2,3,4,5,6,7,8,9],\n",
    "                n_filters = [15,15,30,35,35,30,15,15,15],\n",
    "                hidden_sizes = [64,16,4])\n",
    "\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr = LR, weight_decay = WEIGHT_DECAY)\n",
    "  model = model.to(device)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_mape = train(model, train_iterator, optimizer)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train MAPE: {train_mape:.2f}%')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xX54yKHDt6dM",
    "outputId": "93fb1052-c085-40f1-9604-82ef423bdeb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 78.594 | Train MAPE: 78.59%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 51.260 | Train MAPE: 51.26%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 46.976 | Train MAPE: 46.98%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 44.093 | Train MAPE: 44.09%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 42.291 | Train MAPE: 42.29%\n",
      "Epoch: 06 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 40.912 | Train MAPE: 40.91%\n",
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 40.132 | Train MAPE: 40.13%\n",
      "Epoch: 08 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 39.359 | Train MAPE: 39.36%\n",
      "Epoch: 09 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 38.564 | Train MAPE: 38.56%\n",
      "Epoch: 10 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 37.633 | Train MAPE: 37.63%\n",
      "Epoch: 11 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 37.000 | Train MAPE: 37.00%\n",
      "Epoch: 12 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 36.112 | Train MAPE: 36.11%\n",
      "Epoch: 13 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 35.579 | Train MAPE: 35.58%\n",
      "Epoch: 14 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 35.000 | Train MAPE: 35.00%\n",
      "Epoch: 15 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 34.694 | Train MAPE: 34.69%\n",
      "Epoch: 16 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 34.264 | Train MAPE: 34.26%\n",
      "Epoch: 17 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.944 | Train MAPE: 33.94%\n",
      "Epoch: 18 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.745 | Train MAPE: 33.75%\n",
      "Epoch: 19 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.441 | Train MAPE: 33.44%\n",
      "Epoch: 20 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.138 | Train MAPE: 33.14%\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 73.809 | Train MAPE: 73.81%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 47.519 | Train MAPE: 47.52%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 43.366 | Train MAPE: 43.37%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 41.845 | Train MAPE: 41.85%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 40.633 | Train MAPE: 40.63%\n",
      "Epoch: 06 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 39.751 | Train MAPE: 39.75%\n",
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 39.017 | Train MAPE: 39.02%\n",
      "Epoch: 08 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 38.092 | Train MAPE: 38.09%\n",
      "Epoch: 09 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 37.488 | Train MAPE: 37.49%\n",
      "Epoch: 10 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 37.018 | Train MAPE: 37.02%\n",
      "Epoch: 11 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 36.458 | Train MAPE: 36.46%\n",
      "Epoch: 12 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 36.016 | Train MAPE: 36.02%\n",
      "Epoch: 13 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 35.350 | Train MAPE: 35.35%\n",
      "Epoch: 14 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 35.344 | Train MAPE: 35.34%\n",
      "Epoch: 15 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 34.830 | Train MAPE: 34.83%\n",
      "Epoch: 16 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 34.583 | Train MAPE: 34.58%\n",
      "Epoch: 17 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 34.235 | Train MAPE: 34.23%\n",
      "Epoch: 18 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.874 | Train MAPE: 33.87%\n",
      "Epoch: 19 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.772 | Train MAPE: 33.77%\n",
      "Epoch: 20 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.646 | Train MAPE: 33.65%\n",
      "Epoch: 21 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.436 | Train MAPE: 33.44%\n",
      "Epoch: 22 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.313 | Train MAPE: 33.31%\n",
      "Epoch: 23 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.150 | Train MAPE: 33.15%\n",
      "Epoch: 24 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.901 | Train MAPE: 32.90%\n",
      "Epoch: 25 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.890 | Train MAPE: 32.89%\n",
      "Epoch: 26 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.730 | Train MAPE: 32.73%\n",
      "Epoch: 27 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.493 | Train MAPE: 32.49%\n",
      "Epoch: 28 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.452 | Train MAPE: 32.45%\n",
      "Epoch: 29 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.186 | Train MAPE: 32.19%\n",
      "Epoch: 30 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.073 | Train MAPE: 32.07%\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 83.008 | Train MAPE: 83.01%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 47.560 | Train MAPE: 47.56%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 43.355 | Train MAPE: 43.35%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 40.988 | Train MAPE: 40.99%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 39.645 | Train MAPE: 39.65%\n",
      "Epoch: 06 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 38.543 | Train MAPE: 38.54%\n",
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 37.534 | Train MAPE: 37.53%\n",
      "Epoch: 08 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 36.747 | Train MAPE: 36.75%\n",
      "Epoch: 09 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 36.358 | Train MAPE: 36.36%\n",
      "Epoch: 10 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 35.870 | Train MAPE: 35.87%\n",
      "Epoch: 11 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 35.368 | Train MAPE: 35.37%\n",
      "Epoch: 12 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 34.908 | Train MAPE: 34.91%\n",
      "Epoch: 13 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 34.556 | Train MAPE: 34.56%\n",
      "Epoch: 14 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 34.222 | Train MAPE: 34.22%\n",
      "Epoch: 15 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.853 | Train MAPE: 33.85%\n",
      "Epoch: 16 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.857 | Train MAPE: 33.86%\n",
      "Epoch: 17 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.422 | Train MAPE: 33.42%\n",
      "Epoch: 18 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.211 | Train MAPE: 33.21%\n",
      "Epoch: 19 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 33.017 | Train MAPE: 33.02%\n",
      "Epoch: 20 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.925 | Train MAPE: 32.92%\n",
      "Epoch: 21 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.773 | Train MAPE: 32.77%\n",
      "Epoch: 22 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.706 | Train MAPE: 32.71%\n",
      "Epoch: 23 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.540 | Train MAPE: 32.54%\n",
      "Epoch: 24 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.407 | Train MAPE: 32.41%\n",
      "Epoch: 25 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 32.093 | Train MAPE: 32.09%\n",
      "Epoch: 26 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.974 | Train MAPE: 31.97%\n",
      "Epoch: 27 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.985 | Train MAPE: 31.98%\n",
      "Epoch: 28 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.923 | Train MAPE: 31.92%\n",
      "Epoch: 29 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.811 | Train MAPE: 31.81%\n",
      "Epoch: 30 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.538 | Train MAPE: 31.54%\n",
      "Epoch: 31 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.687 | Train MAPE: 31.69%\n",
      "Epoch: 32 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.509 | Train MAPE: 31.51%\n",
      "Epoch: 33 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.276 | Train MAPE: 31.28%\n",
      "Epoch: 34 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.107 | Train MAPE: 31.11%\n",
      "Epoch: 35 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.159 | Train MAPE: 31.16%\n",
      "Epoch: 36 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.099 | Train MAPE: 31.10%\n",
      "Epoch: 37 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 31.083 | Train MAPE: 31.08%\n",
      "Epoch: 38 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 30.951 | Train MAPE: 30.95%\n",
      "Epoch: 39 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 30.813 | Train MAPE: 30.81%\n",
      "Epoch: 40 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 30.638 | Train MAPE: 30.64%\n"
     ]
    }
   ],
   "source": [
    "models_list = [final_train(whole_train_iterator, epochs) for epochs in [20,30,40]] # 20,30,40 training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiCiMEXq4BEs"
   },
   "source": [
    "## Make A Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "VyW-zwwQ4zJd"
   },
   "outputs": [],
   "source": [
    "test_final = pd.read_csv(os.path.join(data_path, \"text_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "4m_VR14J5sxg"
   },
   "outputs": [],
   "source": [
    "def get_idx_tokens(data):\n",
    "  \"\"\"\n",
    "  Getting index from text data\n",
    "  \"\"\"\n",
    "\n",
    "  tokens = data.iloc[:,0].str.split(\" \")\n",
    "  max_len =tokens.apply(len).max()\n",
    "  word_to_idx = dict(PRODUCT.vocab.stoi)\n",
    "  \n",
    "  def get_matching_idx(lst):\n",
    "    res = []\n",
    "    for i in lst:\n",
    "      if i in word_to_idx.keys():\n",
    "        res.append(word_to_idx[i])\n",
    "      else:\n",
    "        res.append(0)\n",
    "    return res\n",
    "  token_idx = tokens.apply(lambda x: get_matching_idx(x))\n",
    "  token_idx_padded = token_idx.apply(lambda x: x + [1] * (max_len - len(x)))\n",
    "\n",
    "  return torch.LongTensor(token_idx_padded).to(device)\n",
    "\n",
    "def get_predict(seq, model):\n",
    "  \"\"\"\n",
    "  make predictions\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "\n",
    "    predictions = model(seq).squeeze(1)\n",
    "\n",
    "  return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Pru5X8CDSinm"
   },
   "outputs": [],
   "source": [
    "# get_predict\n",
    "val_padded = get_idx_tokens(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "_Dtc2HFhEjXC"
   },
   "outputs": [],
   "source": [
    "unk_mask = pd.Series(val_padded.cpu()).apply(lambda x: x.count(0) / len(x) >= 0.3 ) # unknown token 비율이 0.3이상인 데이터를 걸러냄\n",
    "unk_mask_idx = np.where(unk_mask == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "FuxHI3G-Eiy2"
   },
   "outputs": [],
   "source": [
    "res = np.array([get_predict(val_padded, mdl) for mdl in models_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "cO2_bDWU05F6"
   },
   "outputs": [],
   "source": [
    "res_avg = res.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "dbpGMuc6OVKn"
   },
   "outputs": [],
   "source": [
    "# Writing results\n",
    "\n",
    "test_final[\"취급액\"] = res_avg\n",
    "test_final[\"취급액\"].to_csv(os.path.join(output_path, \"text_cnn_final_predictions.csv\"), index = False)\n",
    "\n",
    "with open(os.path.join(output_path, \"unk_token_masking.pickle\"), \"wb\") as f:\n",
    "  pickle.dump(unk_mask_idx, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "opxL7OvYNzh6"
   ],
   "machine_shape": "hm",
   "name": "TextCNN_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
